{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6ayCViEg4nh"
   },
   "source": [
    "#### **Welcome to Assignment 2 on Deep Learning for Computer Vision.**\n",
    "This assignment consists of three parts. Part-1 is based on the content you learned in Week-3 of course and Part-2 is based on the content you learned in Week-4 of the course. Part-3 is **un-graded** and mainly designed to help you flex the Deep Learning muscles grown in Part-2. \n",
    "\n",
    "Unlike the first two parts, you'll have to implement everything from scratch in Part-3. If you find answers to questions in Part-3, feel free to head out to the forums and discuss them with your classmates!\n",
    "\n",
    "#### **Instructions**\n",
    "1. Use Python 3.x to run this notebook\n",
    "2. Write your code only in between the lines 'YOUR CODE STARTS HERE' and 'YOUR CODE ENDS HERE'.\n",
    "you should not change anything else in the code cells, if you do, the answers you are supposed to get at the end of this assignment might be wrong.\n",
    "3. Read documentation of each function carefully.\n",
    "4. All the Best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jca6tcpSh0EK"
   },
   "source": [
    "# Part-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5KURep5V_0th",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# %matplotlib inline uncomment this line if you're running this notebook on your local PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "SZEae-LT_5n-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhErU_TY-PpU"
   },
   "source": [
    "### Question 1: Point matching using RANSAC\n",
    "\n",
    "Given two sets of points related by affine transformation(with an outlier rate), use the RANSAC method to estimate the Affine transformation parameters between them and the number of inliers(Matching points).\n",
    "\n",
    "Which of the following is the estimated number of inliers for an outlier rate of 0.7:\n",
    "\n",
    "1. 76\n",
    "2. 157\n",
    "3. 223\n",
    "4. 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TYotkDxG-kkP",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:124: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:132: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118.60503371408957 1000\n",
      "43.481575740161496 1000\n",
      "42.57823280539544 1000\n",
      "40.945143546052 1000\n",
      "40.70093593034735 1000\n",
      "39.79358946975772 1000\n",
      "32.968839571040185 1000\n",
      "32.96883957104018 1000\n",
      "32.96883957104017 1000\n",
      "32.96883957104012 1000\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# Affine Transform\n",
    "# |x'|  = |a, b| * |x|  +  |tx|\n",
    "# |y'|    |c, d|   |y|     |ty|\n",
    "# pts_t =    A   * pts_s  + t\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Test Class Affine\n",
    "\n",
    "class Affine_Transform():\n",
    "\n",
    "    def create_test_case(self, outlier_rate=0):\n",
    "        ''' CREATE_TEST_CASE\n",
    "\n",
    "            Randomly generate a test case of affine transformation.\n",
    "\n",
    "            Input arguments:\n",
    "\n",
    "            - outlier_rate : the percentage of outliers in test case,\n",
    "            default is 0\n",
    "\n",
    "            Outputs:\n",
    "\n",
    "            - pts_s : Source points that will be transformed\n",
    "            - pts_t : warped points\n",
    "            - A, t : parameters of affine transformation, A is a 2x2\n",
    "            matrix, t is a 2x1 vector, both of them are created randomly\n",
    "\n",
    "        '''\n",
    "\n",
    "        # Randomly generate affine transformation\n",
    "        # A is a 2x2 matrix, the range of each value is from -2 to 2\n",
    "        A = 4 * np.random.rand(2, 2) - 2\n",
    "\n",
    "        # % t is a 2x1 VECTOR, the range of each value is from -10 to 10\n",
    "        t = 20 * np.random.rand(2, 1) - 10\n",
    "\n",
    "        # Set the number of points in test case\n",
    "        num = 1000\n",
    "\n",
    "        # Compute the number of outliers and inliers respectively\n",
    "        outliers = int(np.round(num * outlier_rate))\n",
    "        inliers = int(num - outliers)\n",
    "\n",
    "        # Gernerate source points whose scope from (0,0) to (100, 100)\n",
    "        pts_s = 100 * np.random.rand(2, num)\n",
    "        # Initialize warped points matrix\n",
    "        pts_t = np.zeros((2, num))\n",
    "\n",
    "        # Compute inliers in warped points matrix by applying A and t\n",
    "        pts_t[:, :inliers] = np.dot(A, pts_s[:, :inliers]) + t\n",
    "\n",
    "        # Generate outliers in warped points matrix\n",
    "        pts_t[:, inliers:] = 100 * np.random.rand(2, outliers)\n",
    "\n",
    "        # Reset the order of warped points matrix,\n",
    "        # outliers and inliers will scatter randomly in test case\n",
    "        rnd_idx = np.random.permutation(num)\n",
    "        pts_s = pts_s[:, rnd_idx]\n",
    "        pts_t = pts_t[:, rnd_idx]\n",
    "\n",
    "        return A, t, pts_s, pts_t\n",
    "\n",
    "    def estimate_affine(self, pts_s, pts_t):\n",
    "        ''' ESTIMATE_AFFINE\n",
    "\n",
    "            Estimate affine transformation by the given points\n",
    "            correspondences.\n",
    "\n",
    "            Input arguments:\n",
    "            - pts_t : points in target image\n",
    "            - pts_s : points in source image\n",
    "\n",
    "            Outputs:\n",
    "\n",
    "            - A, t : the affine transformation, A is a 2x2 matrix\n",
    "            that indicates the rotation and scaling transformation,\n",
    "            t is a 2x1 vector determines the translation\n",
    "\n",
    "            Method:\n",
    "\n",
    "            To estimate an affine transformation between two images,\n",
    "            at least 3 corresponding points are needed.\n",
    "            In this case, 6-parameter affine transformation are taken into\n",
    "            consideration, which is shown as follows:\n",
    "\n",
    "            | x' | = | a b | * | x | + | tx |\n",
    "            | y' |   | c d |   | y |   | ty |\n",
    "\n",
    "            For 3 corresponding points, 6 equations can be formed as below:\n",
    "\n",
    "            | x1 y1 0  0  1 0 |       | a  |       | x1' |\n",
    "            | 0  0  x1 y1 0 1 |       | b  |       | y1' |\n",
    "            | x2 y2 0  0  1 0 |   *   | c  |   =   | x2' |\n",
    "            | 0  0  x2 y2 0 1 |       | d  |       | y2' |\n",
    "            | x3 y3 0  0  1 0 |       | tx |       | x3' |\n",
    "            | 0  0  x3 y3 0 1 |       | ty |       | y3' |\n",
    "\n",
    "            |------> M <------|   |-> theta <-|   |-> b <-|\n",
    "\n",
    "            Solve the equation to compute theta by:  theta = M \\ b\n",
    "            Thus, affine transformation can be obtained as:\n",
    "\n",
    "            A = | a b |     t = | tx |\n",
    "                | c d |         | ty |\n",
    "\n",
    "        '''\n",
    "\n",
    "        # Get the number of corresponding points\n",
    "        pts_num = pts_s.shape[1]\n",
    "\n",
    "        # Initialize the matrix M,\n",
    "        # M has 6 columns, since the affine transformation\n",
    "        # has 6 parameters in this case\n",
    "        M = np.zeros((2 * pts_num, 6))\n",
    "\n",
    "        for i in range(pts_num):\n",
    "            # Form the matrix m\n",
    "            temp = [[pts_s[0, i], pts_s[1, i], 0, 0, 1, 0],\n",
    "                    [0, 0, pts_s[0, i], pts_s[1, i], 0, 1]]\n",
    "            M[2 * i: 2 * i + 2, :] = np.array(temp)\n",
    "\n",
    "        # Form the matrix b,\n",
    "        # b contains all known target points\n",
    "        b = pts_t.T.reshape((2 * pts_num, 1))\n",
    "\n",
    "        try:\n",
    "            # Solve the linear equation\n",
    "            theta = np.linalg.lstsq(M, b)[0]\n",
    "\n",
    "            # Form the affine transformation\n",
    "            A = theta[:4].reshape((2, 2))\n",
    "            t = theta[4:]\n",
    "        except np.linalg.linalg.LinAlgError:\n",
    "            # If M is singular matrix, return None\n",
    "            # print(\"Singular matrix.\")\n",
    "            A = None\n",
    "            t = None\n",
    "\n",
    "        return A, t\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# Create instance\n",
    "af = Affine_Transform()\n",
    "\n",
    "# Generate a test case as validation with\n",
    "# a rate of outliers\n",
    "### YOUR CODE STARTS HERE\n",
    "outlier_rate = 0.7\n",
    "### YOUR CODE ENDS HERE\n",
    "A_true, t_true, pts_s, pts_t = af.create_test_case(outlier_rate)\n",
    "\n",
    "# At least 3 corresponding points to\n",
    "# estimate affine transformation\n",
    "K = 4\n",
    "# Randomly select 3 pairs of points to do estimation\n",
    "idx = np.random.randint(0, pts_s.shape[1], (K, 1))\n",
    "\n",
    "A_test, t_test = af.estimate_affine(pts_s[:, idx], pts_t[:, idx])\n",
    "\n",
    "# Display known parameters with estimations\n",
    "# They should be same when outlier_rate equals to 0,\n",
    "# otherwise, they are totally different in some cases\n",
    "#print(A_true, '\\n', t_true)\n",
    "#print(A_test, '\\n', t_test)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Test Class Ransac\n",
    "# The number of iterations in RANSAC\n",
    "ITER_NUM = 2000\n",
    "\n",
    "\n",
    "class Ransac():\n",
    "\n",
    "    def __init__(self, K=3, threshold=1):\n",
    "        ''' __INIT__\n",
    "\n",
    "            Initialize the instance.\n",
    "\n",
    "            Input argements:\n",
    "\n",
    "            - K : the number of corresponding points,\n",
    "            default is 3\n",
    "            - threshold : determing which points are inliers\n",
    "            by comparing residual with it\n",
    "\n",
    "        '''\n",
    "\n",
    "        self.K = K\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def residual_lengths(self, A, t, pts_s, pts_t):\n",
    "        ''' RESIDUAL_LENGTHS\n",
    "\n",
    "            Compute residual length (Euclidean distance) between\n",
    "            estimation and real target points. Estimation are\n",
    "            calculated by the given source point and affine\n",
    "            transformation (A & t).\n",
    "\n",
    "            Input arguments:\n",
    "\n",
    "            - A, t : the estimated affine transformation calculated\n",
    "            by least squares method\n",
    "            - pts_s : key points from source image\n",
    "            - pts_t : key points from target image\n",
    "\n",
    "            Output:\n",
    "\n",
    "            - residual : Euclidean distance between estimated points\n",
    "            and real target points\n",
    "\n",
    "        '''\n",
    "\n",
    "        if not(A is None) and not(t is None):\n",
    "            # Calculate estimated points:\n",
    "            # pts_e = A * pts_s + t\n",
    "            pts_e = np.dot(A, pts_s) + t\n",
    "\n",
    "            # Calculate the residual length between estimated points\n",
    "            # and target points\n",
    "            diff_square = np.power(pts_e - pts_t, 2)\n",
    "            residual = np.sqrt(np.sum(diff_square, axis=0))\n",
    "            #print(residual.shape)\n",
    "        else:\n",
    "            residual = None\n",
    "\n",
    "        return residual\n",
    "\n",
    "    def ransac_fit(self, pts_s, pts_t):\n",
    "        ''' RANSAC_FIT\n",
    "\n",
    "            Apply the method of RANSAC to obtain the estimation of\n",
    "            affine transformation and inliers as well.\n",
    "\n",
    "            Input arguments:\n",
    "\n",
    "            - pts_s : key points from source image\n",
    "            - pts_t : key points from target image\n",
    "\n",
    "            Output:\n",
    "\n",
    "            - A, t : estimated affine transformation\n",
    "            - inliers : indices of inliers that will be applied to refine the\n",
    "            affine transformation\n",
    "\n",
    "        '''\n",
    "        min_residual=1000\n",
    "        pts_num = pts_s.shape[1]\n",
    "        \n",
    "        #### YOUR CODE START HERE\n",
    "        for i in range(ITER_NUM):\n",
    "            idx = np.random.randint(0, pts_s.shape[1], (self.K, 1))\n",
    "            #print(idx)\n",
    "            A, t = af.estimate_affine(pts_s[:, idx], pts_t[:, idx])\n",
    "            residual = self.residual_lengths(A, t, pts_s, pts_t)\n",
    "            avg_res=np.sum(residual)/pts_num\n",
    "            if  avg_res< min_residual:\n",
    "                min_residual=avg_res\n",
    "                print(min_residual,pts_num)\n",
    "                inliers=[]\n",
    "                for j in range(pts_num):\n",
    "                    out = np.matmul(A,pts_s[:,j].reshape(2,1)) + t\n",
    "                    #print(out.shape,pts_s[:,j].reshape(2,1).shape,t.shape)\n",
    "                    res=np.sqrt(np.sum(np.power(out-pts_t[:,j].reshape(2,1),2)))\n",
    "                    if  res< self.threshold:\n",
    "                        inliers.append(j)\n",
    "        \n",
    "        \n",
    "        ### YOUR CODE ENDS HERE\n",
    "        return A, t, np.array(inliers)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# Create instance\n",
    "rs = Ransac(K=3, threshold=1)\n",
    "\n",
    "residual = rs.residual_lengths(A_test, t_test, pts_s, pts_t)\n",
    "\n",
    "# Run RANSAC to estimate affine transformation when\n",
    "# too many outliers in points set\n",
    "A_rsc, t_rsc, inliers = rs.ransac_fit(pts_s, pts_t)\n",
    "\n",
    "# print the number of inliners or point matches\n",
    "print (inliers.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnf_pn2N-vS7"
   },
   "source": [
    "### Question 2: Detect corners in a given image using Harris Corner Detection Algorithm\n",
    "\n",
    "Find the number of detected corner points in a given image using Harris Corner Detection Algorithm. Note that, Following criterion MUST be satisfied while applying Harris Corner detection Algorithm:\n",
    "\n",
    "1. The size of neighbourhood considered for corner detection = 2.\n",
    "2. Aperture parameter of Sobel derivative used = 3.\n",
    "3. Harris detector free parameter in the equation = 0.04.\n",
    "\n",
    "How many corners are detected?\n",
    "\n",
    "1. 1068\n",
    "2. 780\n",
    "3. 1106\n",
    "4. 976"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "LQSynG8c-0y4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1068\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in the image\n",
    "image = cv2.imread('image.png')\n",
    "\n",
    "# Make a copy of the image\n",
    "image_copy = np.copy(image)\n",
    "\n",
    "# Change color to RGB (from BGR)\n",
    "image_copy = cv2.cvtColor(image_copy, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "###YOUR CODE STARTS HERE\n",
    "\n",
    "## STEP 1:  Convert to grayscale \n",
    "\n",
    "gray_image = cv2.cvtColor(image_copy, cv2.COLOR_RGB2GRAY)\n",
    "## STEP 2: Detect corners \n",
    "dest_initial = cv2.cornerHarris(gray_image, 2, 3, 0.04)\n",
    "## STEP 3: Dilate corner image to enhance corner points\n",
    "dest = cv2.dilate(dest_initial, None)\n",
    "## STEP 4:set threshold value as 0.1 * (maximum value of dilated corner image obtained from STEP3)\n",
    "image[dest > 0.1 * dest.max()]=[0, 0, 255]\n",
    "cv2.imshow('Image with Corners', image)\n",
    "#cv2.waitKey(0) \n",
    "#cv2.destroyAllWindows()\n",
    "## STEP 5: Count numer of detected corner points and draw them on the image\n",
    "num_corners = np.sum(dest > 0.1 * dest.max())\n",
    "print(num_corners)\n",
    "### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSGQF0Zu9Vuo"
   },
   "source": [
    "## Line detection from a given image. (Optional)\n",
    "\n",
    "\n",
    "Find the starting and ending point co-ordinates of detected lines of a given image using hough transform. \n",
    "\n",
    "Following criterion need to be satisfied to qualify as a line:\n",
    "\n",
    "1. Minimum line length = 100;\n",
    "2. Maximum allowed gap between line segments = 200;\n",
    "3. Accumulator threshold parameter = 50  (only those lines are returned that get enough votes);\n",
    "4. Distance resolution of the accumulator in pixels = 1;\n",
    "5. Angle resolution of the accumulator in radians = pi/180\n",
    "\n",
    "\n",
    "Which is the mean of the start and end points of all the detected lines?\n",
    "\n",
    "1. [324.6,  37.6], [490.4,  81.2]\n",
    "2. [314.2, 34.2], [489.1,  76.4]\n",
    "3. [312.9, 39.4], [492.3,  77.1]\n",
    "4. None of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwdLOgZs99bC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read image \n",
    "img = cv2.imread('image.png', cv2.IMREAD_COLOR)\n",
    "\n",
    "# Visualize the input image\n",
    "plt.imshow(img)\n",
    "plt.title('Input Image')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#convert the image to gray-scale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Find the edges in the image using canny detector\n",
    "\n",
    "from skimage import feature\n",
    "edges = cv2.Canny(gray, 200, 300)\n",
    "\n",
    "#### YOUR CODE STARTS HERE #####\n",
    "\n",
    "\n",
    "#### YOUR CODE ENDS HERE #####\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.title('Detected Line Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXrQtCnwhz5J"
   },
   "source": [
    "# Part-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FVEgO_bNhLtx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import timeit\n",
    "import unittest\n",
    "\n",
    "## Please DONOT remove these lines. \n",
    "torch.manual_seed(2021)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mc4Nl4gCLyiN"
   },
   "source": [
    "### Data Loading and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ECXvC1tUhpPA",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# check availability of GPU and set the device accordingly\n",
    "#### YOUR CODE STARTS HERE ####\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#### YOUR CODE ENDS HERE ####\n",
    "\n",
    "# define a transforms for preparing the dataset\n",
    "# for normalization of the MNIST dataset, take mean=0.1307 and std=0.3081\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,),(0.3081,)),])\n",
    "      # convert the image to a pytorch tensor\n",
    "      # normalise the images with mean and std of the dataset (mean = 0.1307, std=0.3081)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1e88bU4vhtdG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the MNIST training, test datasets using `torchvision.datasets.MNIST` using the transform defined above\n",
    "#### YOUR CODE STARTS HERE ####\n",
    "train_dataset = datasets.MNIST('~/.pytorch/MNIST_data/',download=True,train=True,transform=transform)\n",
    "test_dataset = datasets.MNIST('~/.pytorch/MNIST_data/',download=True,train=False,transform=transform)\n",
    "#### YOUR CODE ENDS HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "r2eBGz9OhvD1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create dataloaders for training and test datasets\n",
    "# use a batch size of 32 and set shuffle=True for the training set\n",
    "#### YOUR CODE STARTS HERE ####\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=32,shuffle=True)\n",
    "#### YOUR CODE ENDS HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFfzN-oaL762"
   },
   "source": [
    "### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FPQMmDKbhwy9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # define a linear layer with output channels as 1024\n",
    "        self.linear1 = nn.Linear(784,1024)\n",
    "        # define a linear layer with output channels as 512\n",
    "        self.linear2 = nn.Linear(1024,512)\n",
    "        # define a linear layer with output channels as 256\n",
    "        self.linear3 = nn.Linear(512,256)\n",
    "        # define dropout layer with a probability of 0.25\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        # define a linear layer with 128 output features\n",
    "        self.linear4 = nn.Linear(256,128)\n",
    "        # define a linear layer with output features corresponding to the number of classes in the dataset\n",
    "        self.linear5 = nn.Linear(128,10)\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the layers defined above in a sequential way (folow the same as the layer definitions above) and \n",
    "        # write the forward pass, after each of linear1, linear2, linear3, and linear4 use a relu activation. \n",
    "        # don't forget to resize your input x\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x= self.linear1(x)\n",
    "        x= F.relu(x)\n",
    "        x= self.linear2(x)\n",
    "        x= F.relu(x)\n",
    "        x= self.linear3(x)\n",
    "        x= F.relu(x)\n",
    "        x= self.dropout1(x)\n",
    "        x= self.linear4(x)\n",
    "        x= F.relu(x)\n",
    "        x= self.linear5(x)\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4evsB1kA914e"
   },
   "source": [
    "### Question 3\n",
    "\n",
    "What are total number of parameters in the model? \n",
    "\n",
    "1. 1654932\n",
    "2. 1852197\n",
    "3. 1494154\n",
    "4. 2259843"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNxqjKUH914e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE STARTS HERE ####\n",
    "Network =Net()\n",
    "parameters = Network.linear1.weight.numel() + Network.linear2.weight.numel() + Network.linear3.weight.numel() + Network.linear4.weight.numel() + Network.linear5.weight.numel() + Network.linear1.bias.numel() + Network.linear2.bias.numel() + Network.linear3.bias.numel() + Network.linear4.bias.numel() + Network.linear5.bias.numel()\n",
    "print(parameters)\n",
    "pytorch_total_params = sum(p.numel() for p in Network.parameters())\n",
    "print(pytorch_total_params)\n",
    "#print(Network.parameters())\n",
    "#### YOUR CODE ENDS HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_8IZKqGiAhM"
   },
   "source": [
    "### Sanity Check\n",
    "Make sure all the tests below pass without any errors, before you proceed with the training part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KAiwx-TSyPK6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestImplementations(unittest.TestCase):\n",
    "    \n",
    "    # Dataloading tests\n",
    "    def test_dataset(self):\n",
    "        self.dataset_classes = ['0 - zero',\n",
    "                                '1 - one',\n",
    "                                '2 - two',\n",
    "                                '3 - three',\n",
    "                                '4 - four',\n",
    "                                '5 - five',\n",
    "                                '6 - six',\n",
    "                                '7 - seven',\n",
    "                                '8 - eight',\n",
    "                                '9 - nine']\n",
    "        self.assertTrue(train_dataset.classes == self.dataset_classes)\n",
    "        self.assertTrue(train_dataset.train == True)\n",
    "    \n",
    "    def test_dataloader(self):        \n",
    "        self.assertTrue(train_dataloader.batch_size == 32)\n",
    "        self.assertTrue(test_dataloader.batch_size == 32)      \n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromModule(TestImplementations())\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GkrWn_dfVyo"
   },
   "source": [
    "### Training and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IIdgIfkAGXi9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "      #### YOUR CODE STARTS HERE ####\n",
    "        # send the image, target to the device\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        # flush out the gradients stored in optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # pass the image to the model and assign the output to variable named output\n",
    "        output=model.forward(data)\n",
    "        # calculate the loss (use nll_loss in pytorch)\n",
    "        loss = F.nll_loss(output,target)\n",
    "        # do a backward pass\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "      #### YOUR CODE ENDS HERE ####\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jWEpNBtdHVWr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "          ### YOUR CODE STARTS HERE ####\n",
    "            # send the image, target to the device\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # pass the image to the model and assign the output to variable named output\n",
    "            output=model.forward(data)\n",
    "          #### YOUR CODE ENDS HERE ####\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m02MLg2mxRwB"
   },
   "source": [
    "### Question 4\n",
    "\n",
    "Run the code cell below and report the final test accuracy (If you are not getting the exact number shown in options, please report the closest number).\n",
    "1. 58%\n",
    "2. 69%\n",
    "3. 97%\n",
    "4. 89%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRebLHiriRsu",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "\n",
    "## Define Adam Optimiser with a learning rate of 0.0001\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.0001)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "for epoch in range(1, 4):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)\n",
    "stop = timeit.default_timer()\n",
    "print('Total time taken: {} seconds'.format(int(stop - start)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TV3DSN3v0NBN"
   },
   "source": [
    "### Question 5\n",
    "\n",
    "Modify the network to replace ReLU activations with Sigmoid and report the final test accuracy by running the cell below. (If you are not getting the exact number shown in options, please report the closest number). \n",
    "\n",
    "1. 48%\n",
    "2. 11%\n",
    "3. 39%\n",
    "4. 69%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "t-2mgA4T0KeU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NetSigmoid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetSigmoid, self).__init__()\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "\n",
    "        # define a linear layer with output channels as 1024\n",
    "        self.linear1 = nn.Linear(784,1024)\n",
    "        # define a linear layer with output channels as 512\n",
    "        self.linear2 = nn.Linear(1024,512)\n",
    "        # define a linear layer with output channels as 256\n",
    "        self.linear3 = nn.Linear(512,256)\n",
    "        # define dropout layer with a probability of 0.25\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        # define a linear layer with 128 output features\n",
    "        self.linear4 = nn.Linear(256,128)\n",
    "        # define a linear layer with output features corresponding to the number of classes in the dataset\n",
    "        self.linear5 = nn.Linear(128,10)\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x= self.linear1(x)\n",
    "        x= F.sigmoid(x)\n",
    "        x= self.linear2(x)\n",
    "        x= F.sigmoid(x)\n",
    "        x= self.linear3(x)\n",
    "        x= F.sigmoid(x)\n",
    "        x= self.dropout1(x)\n",
    "        x= self.linear4(x)\n",
    "        x= F.sigmoid(x)\n",
    "        x= self.linear5(x)\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "W0oZc3cTxJ90",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.313662\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.306510\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.281366\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.310192\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.316741\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.287402\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.317593\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.283708\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.308279\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.314922\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.302465\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.298366\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.307067\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.307901\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.285378\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.303483\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.244703\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.219522\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.107795\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.216868\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.043804\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.856583\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.050316\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.150544\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.986671\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.958110\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.861106\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 1.849749\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.897040\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.196008\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.117315\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 1.889972\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.177254\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 1.880150\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.054942\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 1.837056\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.919351\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 1.925638\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 2.106670\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 1.926109\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.035717\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 1.993957\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 1.852106\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 2.005430\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 2.377278\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 1.817349\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 1.848408\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 1.782571\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.925740\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 2.249845\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.130204\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 1.929674\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 1.970172\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 2.153311\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 1.915216\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 1.907026\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 1.945669\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 2.006305\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 1.781421\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 1.940913\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.915442\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 1.850580\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 2.028913\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 1.898031\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 1.854508\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 1.904031\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 2.033192\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 1.869869\n",
      "Train Epoch: 1 [43520/60000 (73%)]\tLoss: 1.730114\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 1.897280\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.964758\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 1.815966\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 1.915717\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 1.873256\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 2.062033\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.833269\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 1.888599\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 1.730038\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 1.999976\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 1.990268\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.987741\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 1.906911\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 1.745603\n",
      "Train Epoch: 1 [53120/60000 (89%)]\tLoss: 2.017224\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.805296\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 1.923698\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 1.991454\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 1.773423\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 2.025795\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 1.921586\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.883549\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 1.897818\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 2.038781\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 1.718534\n",
      "\n",
      "Test set: Average loss: 1.8193, Accuracy: 2055/10000 (21%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.735773\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 1.794619\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 1.950470\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 2.146610\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 2.247383\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 2.024301\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 1.998522\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 2.044595\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 2.144086\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 2.048655\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.085416\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 2.065647\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 2.103050\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 1.975535\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 2.015166\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 2.093606\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 1.865984\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 1.799674\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 1.906750\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 1.811081\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.957697\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 2.062135\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 2.134387\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 2.012249\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 2.135912\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 1.687474\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 1.906145\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 1.908909\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 1.958074\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 2.112597\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.203255\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 1.898074\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 2.013154\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 1.778425\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 1.854139\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 1.717088\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 1.846328\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 1.749297\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 1.988429\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 1.949052\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.308713\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 1.955504\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 1.801008\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 2.171634\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 1.898355\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 2.045718\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 1.879205\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 1.789219\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 2.062615\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 1.970286\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.844226\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 2.131856\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 2.136161\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 2.094423\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 1.945140\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 1.818869\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 1.818551\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 1.902517\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 2.095683\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 2.000896\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.734241\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 2.008719\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 1.770860\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 1.954900\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 2.104828\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 1.892208\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 1.955601\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 1.859744\n",
      "Train Epoch: 2 [43520/60000 (73%)]\tLoss: 1.948127\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 1.826014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.970489\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 2.080476\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 2.034880\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 2.042653\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 1.796540\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 1.700688\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 2.065709\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 2.024115\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 1.794886\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 1.930876\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.962533\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 1.865324\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 1.897191\n",
      "Train Epoch: 2 [53120/60000 (89%)]\tLoss: 1.913742\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 2.009087\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 1.732886\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 2.086964\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 2.043522\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 2.036774\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 2.096804\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.014930\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 2.047335\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 1.743356\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 2.062624\n",
      "\n",
      "Test set: Average loss: 1.8513, Accuracy: 1949/10000 (19%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.950598\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 1.940180\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 1.807967\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 1.885536\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 1.918715\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 1.930159\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 1.844341\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 2.156404\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 1.858139\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 1.861726\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.843156\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 2.026372\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 1.778864\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 1.917353\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 1.992678\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 1.948501\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 1.923251\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 2.165729\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 1.848471\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 1.986761\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.928604\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 2.140108\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 1.921844\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 2.039680\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 1.731655\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 1.986208\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 2.080894\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 1.946488\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 2.206554\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 1.859330\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.793375\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 1.860898\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 1.872725\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 2.078088\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 1.924093\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 1.846024\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 2.043673\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 2.149281\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 2.008516\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 2.044771\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.894833\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 1.832608\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 1.751489\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 2.068932\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 1.765759\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 1.987702\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 1.934086\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 1.833165\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 1.816623\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 1.753978\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.875375\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 1.804996\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 2.060179\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 1.843553\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 2.265210\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 2.056809\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 1.902227\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 1.953097\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 1.919988\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 1.871213\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.898154\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 1.966201\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 1.934885\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 1.814406\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 2.082587\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 1.908668\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 1.855245\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 1.833710\n",
      "Train Epoch: 3 [43520/60000 (73%)]\tLoss: 1.964822\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 2.016256\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.926342\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 1.868890\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 1.838072\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 1.848661\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 1.827820\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 1.870572\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 2.020946\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 1.788663\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 1.866993\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 2.115139\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.115180\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 1.880334\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 1.999826\n",
      "Train Epoch: 3 [53120/60000 (89%)]\tLoss: 1.891949\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 2.187935\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 1.948401\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 2.135673\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 1.865889\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 2.034517\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 1.875950\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 1.704255\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 1.961713\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 1.857665\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 2.148991\n",
      "\n",
      "Test set: Average loss: 1.9538, Accuracy: 2102/10000 (21%)\n",
      "\n",
      "Total time taken: 59 seconds\n"
     ]
    }
   ],
   "source": [
    "model = NetSigmoid().to(device)\n",
    "\n",
    "## Define Adam Optimiser with a learning rate of 0.01\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "for epoch in range(1, 4):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Total time taken: {} seconds'.format(int(stop - start)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lurbs-F35AqX"
   },
   "source": [
    "### Question 6\n",
    "\n",
    "Train the network defined in Question-4 with the same Adam optimizer but change the learning rate to 10. Report the final test accuracy by running the cell below. (If you are not getting the exact number shown in options, please report the closest number). \n",
    "\n",
    "1. 89%\n",
    "2. 97%\n",
    "3. 22%\n",
    "4. 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuddf3Tb5Oi6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "\n",
    "## Define Adam Optimiser with a learning rate of 10\n",
    "optimizer = optim.Adam(model.parameters(),lr=10)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "for epoch in range(1, 4):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Total time taken: {} seconds'.format(int(stop - start)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m34pGNerYmld"
   },
   "source": [
    "### Question 7\n",
    "\n",
    "Modify the network  Question-4 `(Net)` to replace ReLU activations with Tanh and initialise the `Linear` layer weights to zero. Report the final test accuracy by running the cell below. (If you are not getting the exact number shown in options, please report the closest number). \n",
    "\n",
    "1. 11%\n",
    "2. 74%\n",
    "3. 87%\n",
    "4. 99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "itaNhtJX914i",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NetTanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetTanh, self).__init__()\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # define a linear layer with output channels as 1024\n",
    "        self.linear1 = nn.Linear(784,1024)\n",
    "        # define a linear layer with output channels as 512\n",
    "        self.linear2 = nn.Linear(1024,512)\n",
    "        # define a linear layer with output channels as 256\n",
    "        self.linear3 = nn.Linear(512,256)\n",
    "        # define dropout layer with a probability of 0.25\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        # define a linear layer with 128 output features\n",
    "        self.linear4 = nn.Linear(256,128)\n",
    "        # define a linear layer with output features corresponding to the number of classes in the dataset\n",
    "        self.linear5 = nn.Linear(128,10)\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the layers defined above in a sequential way (folow the same as the layer definitions above) and \n",
    "        # write the forward pass, after each of linear1, linear2, linear3 and linear4 use a tanh activation.  \n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x= self.linear1(x)\n",
    "        x= F.tanh(x)\n",
    "        x= self.linear2(x)\n",
    "        x= F.tanh(x)\n",
    "        x= self.linear3(x)\n",
    "        x= F.tanh(x)\n",
    "        x= self.dropout1(x)\n",
    "        x= self.linear4(x)\n",
    "        x= F.tanh(x)\n",
    "        x= self.linear5(x)\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "GsHOjQA_Ylzj",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.309649\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.374694\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.318996\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.487586\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.520357\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.375355\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.331163\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.586874\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.382975\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.458581\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.672601\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.417860\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.479212\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.293718\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.496849\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.453576\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.342081\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.364635\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.286402\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.267293\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.379240\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.443769\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.472769\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.538817\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.465143\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.314795\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.668534\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.328417\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.415470\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.438956\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.338780\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 2.310935\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.282245\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 2.492591\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.425969\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.533377\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 2.624958\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 2.259258\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 2.444154\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 2.725431\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.555192\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 2.405753\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 2.346260\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 2.362679\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 2.428975\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 2.522689\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 2.492542\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 2.371088\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 2.528247\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 2.376141\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.351990\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 2.362288\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 2.417600\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 2.620835\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 2.493784\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 2.381177\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 2.571955\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 2.587465\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 2.542376\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 2.514326\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.443931\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 2.512341\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 2.563820\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 2.361266\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 2.367562\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 2.473795\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 2.579044\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 2.807702\n",
      "Train Epoch: 1 [43520/60000 (73%)]\tLoss: 2.520665\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 2.426273\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.395595\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 2.450193\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 2.551438\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 2.579785\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 2.627140\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 2.612912\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 2.258335\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 2.318892\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 2.509310\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 2.402961\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.392072\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 2.501028\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 2.298036\n",
      "Train Epoch: 1 [53120/60000 (89%)]\tLoss: 2.337343\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 2.399118\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 2.491194\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 2.252741\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 2.257895\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 2.333944\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 2.292611\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.480748\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 2.555335\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 2.292107\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 2.615974\n",
      "\n",
      "Test set: Average loss: 2.4010, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.530515\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 2.469786\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 2.468987\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 2.338067\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 2.477357\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 2.404909\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 2.482846\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 2.379232\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 2.466928\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 2.391513\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.469086\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 2.341474\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 2.352506\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 2.260601\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 2.458854\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 2.443945\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 2.365224\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 2.657974\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 2.298411\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 2.297114\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 3.080311\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 2.741575\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 2.382570\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 2.488907\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 2.388558\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 2.310888\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 2.429738\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 2.232486\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 2.320892\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 2.396197\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.562940\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 2.552815\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 2.366300\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 2.475982\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 2.443411\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 2.499208\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 2.474071\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 2.449696\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 2.469962\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 2.613505\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.329833\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 2.303488\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 2.461022\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 2.458567\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 2.455429\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 2.361009\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 2.534421\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 2.379182\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 2.424930\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 2.402953\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.722391\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 2.528614\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 2.335749\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 2.517373\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 2.446756\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 2.356301\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 2.481357\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 2.482897\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 2.437397\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 2.541174\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.474416\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 2.366240\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 2.518769\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 2.509561\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 2.234627\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 2.474302\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 2.413686\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 2.657805\n",
      "Train Epoch: 2 [43520/60000 (73%)]\tLoss: 2.422075\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 2.427372\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.424241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 2.406654\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 2.432204\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 2.389797\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 2.395518\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 2.386521\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 2.504148\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 2.298756\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 2.486092\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 2.549895\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.408612\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 2.684018\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 2.255903\n",
      "Train Epoch: 2 [53120/60000 (89%)]\tLoss: 2.348781\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 2.323518\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 2.395706\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 2.336519\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 2.302118\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 2.360061\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 2.369080\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.378394\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 2.327749\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 2.395916\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 2.464672\n",
      "\n",
      "Test set: Average loss: 2.3739, Accuracy: 982/10000 (10%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.298532\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 2.337523\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 2.307143\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 2.526026\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 2.400561\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 2.301919\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 2.545395\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 2.490176\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 2.588212\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 2.411838\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 2.618649\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 2.470355\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 2.284869\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 2.313241\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 2.733899\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 2.482405\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 2.392910\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 2.484871\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 2.420443\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 2.301061\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.264460\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 2.526301\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 2.495148\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 2.642422\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 2.367056\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 2.606874\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 2.451293\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 2.413803\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 2.401241\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 2.455987\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.326979\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 2.391771\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 2.553594\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 2.404978\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 2.351862\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 2.427864\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 2.360925\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 2.384365\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 2.491540\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 2.540308\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.424808\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 2.441565\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 2.398545\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 2.407104\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 2.670719\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 2.459236\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 2.354294\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 2.614084\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 2.461350\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 2.426223\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.436009\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 2.452941\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 2.365941\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 2.498179\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 2.401523\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 2.334493\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 2.240946\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 2.685855\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 2.511119\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 2.645766\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.520804\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 2.423613\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 2.376615\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 2.457364\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 2.395146\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 2.367782\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 2.440275\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 2.483856\n",
      "Train Epoch: 3 [43520/60000 (73%)]\tLoss: 2.381144\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 2.594333\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 2.453548\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 2.418541\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 2.327866\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 2.353784\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 2.432773\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 2.387194\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 2.302116\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 2.509153\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 2.354441\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 2.337786\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.323951\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 2.283663\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 2.413439\n",
      "Train Epoch: 3 [53120/60000 (89%)]\tLoss: 2.466211\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 2.327710\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 2.482722\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 2.442794\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 2.510481\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 2.355251\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 2.415262\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 2.517410\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 2.506644\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 2.434782\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 2.486880\n",
      "\n",
      "Test set: Average loss: 2.4268, Accuracy: 982/10000 (10%)\n",
      "\n",
      "Total time taken: 55 seconds\n"
     ]
    }
   ],
   "source": [
    "model = NetTanh().to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "  #### YOUR CODE STARTS HERE ####\n",
    "  if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.zeros_(m.weight)\n",
    "        #m.bias.data.fill_(0)\n",
    "  #### YOUR CODE ENDS HERE ####\n",
    "  \n",
    "\n",
    "model.apply(init_weights)  \n",
    "## Define Adam Optimiser with a learning rate of 0.01\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "for epoch in range(1, 4):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Total time taken: {} seconds'.format(int(stop - start)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHmX6YO_ay9n"
   },
   "source": [
    "### Question 8\n",
    "\n",
    "Initialize the network defined in Question-1 `(Net)` with Xavier's initialization ([torch.nn.init.xavier_normal](https://pytorch.org/docs/stable/nn.init.html))(for bias use zero). Train the network with Adam optimizer and report the final test accuracy by running the cell below. (If you are not getting the exact number shown in options, please report the closest number). \n",
    "\n",
    "\n",
    "1. 82%\n",
    "2. 76%\n",
    "3. 93%\n",
    "4. 69%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ibfj8A9xZsEw",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.437224\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.209423\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.817739\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.717895\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.689133\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.846899\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.810747\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.793608\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.753729\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.750748\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.982091\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.648359\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.684619\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.764485\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.840850\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.406211\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.853993\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.513509\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.704636\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.260891\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.790235\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.124646\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.223626\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.435255\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.311133\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.964623\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.418080\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.435599\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.447282\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.477992\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.390640\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.227257\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.884420\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.847630\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.223756\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.268145\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.398235\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.408200\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.229473\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.211444\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.359290\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.572136\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.217008\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.894034\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.144951\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.471152\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.219283\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.305850\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.914896\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 1.301952\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.204069\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.271391\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.883323\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.163944\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.952581\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.216812\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.585049\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.361580\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.257635\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.182818\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.660695\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.402860\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.579751\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.637680\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.614669\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.634665\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.528674\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.242334\n",
      "Train Epoch: 1 [43520/60000 (73%)]\tLoss: 0.611972\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.907027\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.943217\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.421873\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.352132\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.537574\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.206672\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.413204\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 1.236889\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.139860\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.477701\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.454211\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.313787\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.959619\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.724138\n",
      "Train Epoch: 1 [53120/60000 (89%)]\tLoss: 0.561369\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.325018\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.559762\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.465966\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.577678\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.625512\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.497410\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.650786\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.364948\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.170289\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.600434\n",
      "\n",
      "Test set: Average loss: 0.4401, Accuracy: 9131/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.277911\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.210773\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.239445\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.230488\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.240811\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.177358\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.368358\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.094687\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.603357\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.487619\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.620867\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.408249\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.458044\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.620750\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.348722\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.065045\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.620966\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.138651\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.234465\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.568002\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.062678\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.270098\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.245505\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.138533\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 1.379227\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.412399\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.307573\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.805797\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.816140\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.260489\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.546165\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.185502\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.250386\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.290392\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.597545\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.308817\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.675941\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 1.080509\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.289578\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.245404\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.257773\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.774400\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.285896\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.673098\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.287766\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.313647\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 3.234989\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.133747\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.474579\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.233807\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.216053\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.627346\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.262262\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.450691\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.312869\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.583690\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.285640\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.390459\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.418105\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.180151\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.080551\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.192155\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.216816\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.973371\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.812917\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.875250\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.325003\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.203668\n",
      "Train Epoch: 2 [43520/60000 (73%)]\tLoss: 0.761157\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.306713\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.264566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.491813\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.362099\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.365715\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.479456\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.963112\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.510936\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 1.019687\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.322813\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.302435\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.742731\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.271025\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.314006\n",
      "Train Epoch: 2 [53120/60000 (89%)]\tLoss: 0.326922\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.622448\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.230095\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.290186\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.388972\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.423201\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.099134\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.172746\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.602523\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.234741\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.401317\n",
      "\n",
      "Test set: Average loss: 0.4676, Accuracy: 8898/10000 (89%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.218783\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.492513\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.195162\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.224846\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.451304\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.373935\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.138659\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.632619\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.084030\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 3.824104\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.213218\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.880601\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.343262\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.287869\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.508339\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.734190\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 1.127881\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.375365\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.354159\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.240771\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.436848\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.220889\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.031570\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.420164\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.481111\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.280577\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.392369\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.128820\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.164196\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.304475\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.375172\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.224006\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.296665\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.216793\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.532617\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.741231\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.693815\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.124302\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.103017\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.528418\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.203394\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 1.030092\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.799106\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 1.001773\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.684101\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.430933\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.545677\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.277500\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.179599\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.299710\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.419204\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.185021\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.530130\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.597437\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.431973\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.758381\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.138235\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.502492\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.274298\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.395762\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.235293\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.344781\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.355082\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.359904\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.045930\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.151442\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.518080\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.214031\n",
      "Train Epoch: 3 [43520/60000 (73%)]\tLoss: 0.362011\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.157286\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.852273\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.365943\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.341265\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.142468\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.223263\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.161201\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.438436\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.597459\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.315087\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.920429\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.337001\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.069503\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 1.138527\n",
      "Train Epoch: 3 [53120/60000 (89%)]\tLoss: 0.361691\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.484397\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.418922\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.694443\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.297099\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.816769\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.867018\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.137176\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.268900\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.587829\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.417856\n",
      "\n",
      "Test set: Average loss: 0.3747, Accuracy: 9148/10000 (91%)\n",
      "\n",
      "Total time taken: 54 seconds\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "  #### YOUR CODE STARTS HERE ####\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "  #### YOUR CODE ENDS HERE #### \n",
    "\n",
    "model.apply(init_weights)  \n",
    "## Define Adam Optimiser with a learning rate of 0.01\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "for epoch in range(1, 4):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Total time taken: {} seconds'.format(int(stop - start)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxogDSnVqcjO"
   },
   "source": [
    "# Part-3 (**Optional**)\n",
    "This section is un-graded and purely for practice. \n",
    "\n",
    "Main focus of this part is to help you flex the deep learning muscles built in the above part. You should build a network on the [SVHN dataset](http://ufldl.stanford.edu/housenumbers/). This dataset is similar to MNIST but unlike MNIST, the images are colored and more complex. \n",
    "\n",
    "As of writing this, the state-of-the-art(SoTA) performance on this dataset is 98.98%. You can try to start with the simple network we defined above for the MNSIT dataset(with some modification for dealing with different sized colored images unlike MNIST). But to achive the SoTA performance you need to do a lot of hackery. These are list of few things, we would encourage you to try: \n",
    "\n",
    "- Use data augmentation wisely. Read and understand how to perform the augmentations listed below. \n",
    "    * RandomFlips, Color Jittering\n",
    "    * Cutout, Cutmix\n",
    "    * Mixup\n",
    "    * Auto-augment\n",
    "\n",
    "- Try to use an image and increase the image size using standard image interpolation techniques. Try using tricks like Progressive resizing of images and see if it helps. \n",
    "\n",
    "- After certain number of layers, adding more layer might not be of much help, run experiments on SVHN and see if you observe this. \n",
    "\n",
    "- To understand the difficulties in training deeper networks read this paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "- To improve the performance on SVHN, try using architectures like [ResNet](https://arxiv.org/abs/1512.03385), [DesnseNet](https://arxiv.org/abs/1608.06993) or [EfficientNet](https://arxiv.org/abs/1905.11946). Most of these architectures are available by default in PyTorch.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL4V_Assignment_2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
