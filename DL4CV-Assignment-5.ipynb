{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cxNlfCYzGlf"
   },
   "source": [
    "#### **Welcome to Assignment 5 on Deep Learning for Computer Vision.**\n",
    "This notebook consists of two parts. In Part-1 you'll have to load pre-trained model and generate captions for a couple of Images, for Part-2 you'll have to implement an encoding layer from scratch. Detailed instructions are provided below\n",
    "\n",
    "#### **Instructions**\n",
    "1. Use Python 3.x to run this notebook\n",
    "2. Write your code only in between the lines 'YOUR CODE STARTS HERE' and 'YOUR CODE ENDS HERE'.\n",
    "you should not change anything else in the code cells, if you do, the answers you are supposed to get at the end of this assignment might be wrong.\n",
    "3. Read documentation of each function carefully.\n",
    "4. All the Best!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AY-MEQAE7rU5"
   },
   "source": [
    "### Part-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQqwvU_CwCq6"
   },
   "source": [
    "**Important:** Download `pretrained_model.zip` and `vocap.zip` files into your notebook root folder before you start the assignment. These are share in the following link: https://drive.google.com/drive/folders/12O5lTaofxKHcOlYeEEfzVwidUeEdVdAK?usp=sharing\n",
    "\n",
    "The following cell automatically downloads them for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "750CSjhu985G",
    "outputId": "49126c32-847c-4d87-f681-2eb617e92104"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2021-10-02 12:10:18--  https://raw.githubusercontent.com/chentinghao/download_google_drive/master/download_gdrive.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1763 (1.7K) [text/plain]\n",
      "Saving to: 'download_gdrive.py.1'\n",
      "\n",
      "     0K .                                                     100%  224K=0.008s\n",
      "\n",
      "2021-10-02 12:10:19 (224 KB/s) - 'download_gdrive.py.1' saved [1763/1763]\n",
      "\n",
      "\n",
      "0.00B [00:00, ?B/s]\n",
      "768kB [00:00, 7.72MB/s]\n",
      "1.91MB [00:00, 8.61MB/s]\n",
      "3.03MB [00:00, 6.57MB/s]\n",
      "4.16MB [00:00, 7.57MB/s]\n",
      "5.31MB [00:00, 8.48MB/s]\n",
      "6.12MB [00:01, 2.94MB/s]\n",
      "7.25MB [00:01, 3.79MB/s]\n",
      "8.41MB [00:01, 4.76MB/s]\n",
      "9.28MB [00:02, 2.59MB/s]\n",
      "10.4MB [00:02, 3.38MB/s]\n",
      "11.6MB [00:02, 4.31MB/s]\n",
      "12.5MB [00:03, 2.06MB/s]\n",
      "13.7MB [00:03, 2.73MB/s]\n",
      "14.8MB [00:03, 3.56MB/s]\n",
      "15.7MB [00:04, 2.34MB/s]\n",
      "16.9MB [00:04, 3.09MB/s]\n",
      "18.0MB [00:04, 3.97MB/s]\n",
      "19.0MB [00:05, 2.51MB/s]\n",
      "20.1MB [00:05, 3.28MB/s]\n",
      "21.2MB [00:05, 4.19MB/s]\n",
      "22.2MB [00:06, 2.52MB/s]\n",
      "23.3MB [00:06, 3.30MB/s]\n",
      "24.4MB [00:07, 2.06MB/s]\n",
      "25.5MB [00:07, 2.74MB/s]\n",
      "26.7MB [00:07, 3.57MB/s]\n",
      "27.6MB [00:08, 2.31MB/s]\n",
      "28.8MB [00:08, 3.04MB/s]\n",
      "29.9MB [00:08, 3.91MB/s]\n",
      "30.8MB [00:09, 2.45MB/s]\n",
      "31.9MB [00:09, 3.22MB/s]\n",
      "33.1MB [00:09, 4.12MB/s]\n",
      "34.0MB [00:10, 2.02MB/s]\n",
      "35.1MB [00:10, 2.69MB/s]\n",
      "36.3MB [00:10, 3.51MB/s]\n",
      "37.2MB [00:11, 2.33MB/s]\n",
      "38.3MB [00:11, 3.07MB/s]\n",
      "39.5MB [00:11, 3.95MB/s]\n",
      "40.4MB [00:12, 2.46MB/s]\n",
      "41.6MB [00:12, 3.23MB/s]\n",
      "42.7MB [00:13, 2.43MB/s]\n",
      "43.8MB [00:13, 3.19MB/s]\n",
      "45.0MB [00:13, 4.09MB/s]\n",
      "45.9MB [00:14, 2.02MB/s]\n",
      "47.0MB [00:14, 2.69MB/s]\n",
      "48.1MB [00:14, 3.49MB/s]\n",
      "49.0MB [00:15, 2.32MB/s]\n",
      "50.2MB [00:15, 3.07MB/s]\n",
      "51.4MB [00:15, 3.95MB/s]\n",
      "52.3MB [00:16, 2.50MB/s]\n",
      "53.5MB [00:16, 3.28MB/s]\n",
      "54.7MB [00:16, 4.17MB/s]\n",
      "55.6MB [00:17, 2.04MB/s]\n",
      "56.7MB [00:17, 2.71MB/s]\n",
      "57.8MB [00:17, 3.53MB/s]\n",
      "58.8MB [00:18, 2.34MB/s]\n",
      "59.9MB [00:18, 3.08MB/s]\n",
      "61.0MB [00:19, 2.35MB/s]\n",
      "62.1MB [00:19, 3.09MB/s]\n",
      "63.2MB [00:19, 3.96MB/s]\n",
      "64.1MB [00:20, 2.43MB/s]\n",
      "65.3MB [00:20, 3.19MB/s]\n",
      "66.4MB [00:20, 4.08MB/s]\n",
      "67.3MB [00:21, 2.02MB/s]\n",
      "68.5MB [00:21, 2.69MB/s]\n",
      "69.6MB [00:21, 3.50MB/s]\n",
      "70.5MB [00:22, 2.34MB/s]\n",
      "71.7MB [00:22, 3.08MB/s]\n",
      "72.8MB [00:22, 3.96MB/s]\n",
      "73.8MB [00:23, 2.46MB/s]\n",
      "74.9MB [00:23, 3.23MB/s]\n",
      "76.0MB [00:23, 4.13MB/s]\n",
      "77.0MB [00:24, 2.06MB/s]\n",
      "78.1MB [00:24, 2.75MB/s]\n",
      "79.3MB [00:25, 2.27MB/s]\n",
      "80.4MB [00:25, 3.00MB/s]\n",
      "81.6MB [00:25, 3.87MB/s]\n",
      "82.5MB [00:26, 2.43MB/s]\n",
      "83.7MB [00:26, 3.20MB/s]\n",
      "84.8MB [00:26, 4.10MB/s]\n",
      "85.8MB [00:27, 2.50MB/s]\n",
      "86.9MB [00:27, 3.28MB/s]\n",
      "88.1MB [00:27, 4.18MB/s]\n",
      "89.0MB [00:28, 2.03MB/s]\n",
      "90.2MB [00:28, 2.71MB/s]\n",
      "91.3MB [00:28, 3.53MB/s]\n",
      "92.2MB [00:29, 2.36MB/s]\n",
      "93.4MB [00:29, 3.11MB/s]\n",
      "94.5MB [00:30, 2.44MB/s]\n",
      "95.7MB [00:30, 3.21MB/s]\n",
      "96.8MB [00:30, 4.10MB/s]\n",
      "97.7MB [00:31, 2.46MB/s]\n",
      "98.8MB [00:31, 3.23MB/s]\n",
      "100MB [00:31, 4.13MB/s] \n",
      "101MB [00:32, 2.02MB/s]\n",
      "102MB [00:32, 2.69MB/s]\n",
      "103MB [00:32, 3.50MB/s]\n",
      "104MB [00:33, 2.33MB/s]\n",
      "105MB [00:33, 3.07MB/s]\n",
      "106MB [00:33, 3.95MB/s]\n",
      "107MB [00:34, 2.46MB/s]\n",
      "108MB [00:34, 3.23MB/s]\n",
      "110MB [00:34, 4.13MB/s]\n",
      "110MB [00:35, 2.03MB/s]\n",
      "112MB [00:35, 2.70MB/s]\n",
      "113MB [00:35, 3.51MB/s]\n",
      "114MB [00:36, 2.33MB/s]\n",
      "115MB [00:36, 3.07MB/s]\n",
      "116MB [00:37, 2.34MB/s]\n",
      "117MB [00:37, 3.08MB/s]\n",
      "118MB [00:37, 3.96MB/s]\n",
      "119MB [00:38, 2.42MB/s]\n",
      "120MB [00:38, 3.18MB/s]\n",
      "121MB [00:38, 4.07MB/s]\n",
      "122MB [00:39, 2.02MB/s]\n",
      "123MB [00:39, 2.69MB/s]\n",
      "125MB [00:39, 3.50MB/s]\n",
      "125MB [00:40, 2.33MB/s]\n",
      "127MB [00:40, 3.07MB/s]\n",
      "128MB [00:40, 3.95MB/s]\n",
      "129MB [00:41, 2.50MB/s]\n",
      "130MB [00:41, 3.28MB/s]\n",
      "131MB [00:41, 4.19MB/s]\n",
      "132MB [00:42, 2.08MB/s]\n",
      "133MB [00:42, 2.76MB/s]\n",
      "134MB [00:43, 2.24MB/s]\n",
      "135MB [00:43, 2.96MB/s]\n",
      "137MB [00:43, 3.81MB/s]\n",
      "137MB [00:44, 2.39MB/s]\n",
      "139MB [00:44, 3.14MB/s]\n",
      "140MB [00:44, 4.03MB/s]\n",
      "141MB [00:45, 2.48MB/s]\n",
      "142MB [00:45, 3.25MB/s]\n",
      "143MB [00:45, 4.16MB/s]\n",
      "144MB [00:46, 2.03MB/s]\n",
      "145MB [00:46, 2.70MB/s]\n",
      "146MB [00:46, 3.51MB/s]\n",
      "147MB [00:47, 2.34MB/s]\n",
      "148MB [00:47, 3.08MB/s]\n",
      "149MB [00:47, 3.95MB/s]\n",
      "150MB [00:48, 2.46MB/s]\n",
      "151MB [00:48, 3.23MB/s]\n",
      "152MB [00:49, 2.07MB/s]\n",
      "154MB [00:49, 2.75MB/s]\n",
      "155MB [00:49, 3.57MB/s]\n",
      "156MB [00:50, 2.32MB/s]\n",
      "157MB [00:50, 3.06MB/s]\n",
      "158MB [00:50, 3.92MB/s]\n",
      "159MB [00:51, 2.45MB/s]\n",
      "160MB [00:51, 3.22MB/s]\n",
      "161MB [00:51, 4.10MB/s]\n",
      "162MB [00:52, 2.51MB/s]\n",
      "163MB [00:52, 3.29MB/s]\n",
      "164MB [00:52, 4.19MB/s]\n",
      "165MB [00:53, 2.04MB/s]\n",
      "166MB [00:53, 2.71MB/s]\n",
      "168MB [00:53, 3.53MB/s]\n",
      "168MB [00:54, 2.34MB/s]\n",
      "170MB [00:54, 3.09MB/s]\n",
      "171MB [00:55, 2.43MB/s]\n",
      "172MB [00:55, 3.19MB/s]\n",
      "173MB [00:55, 4.08MB/s]\n",
      "174MB [00:56, 2.02MB/s]\n",
      "175MB [00:56, 2.69MB/s]\n",
      "176MB [00:56, 3.49MB/s]\n",
      "177MB [00:57, 2.33MB/s]\n",
      "178MB [00:57, 3.07MB/s]\n",
      "180MB [00:57, 3.94MB/s]\n",
      "180MB [00:58, 2.46MB/s]\n",
      "182MB [00:58, 3.23MB/s]\n",
      "183MB [00:58, 4.14MB/s]\n",
      "184MB [00:59, 2.55MB/s]\n",
      "185MB [00:59, 3.34MB/s]\n",
      "186MB [00:59, 4.26MB/s]\n",
      "187MB [01:00, 2.09MB/s]\n",
      "188MB [01:00, 2.78MB/s]\n",
      "189MB [01:01, 2.19MB/s]\n",
      "190MB [01:01, 2.90MB/s]\n",
      "191MB [01:01, 3.75MB/s]\n",
      "192MB [01:02, 2.41MB/s]\n",
      "193MB [01:02, 3.16MB/s]\n",
      "195MB [01:02, 4.05MB/s]\n",
      "196MB [01:03, 2.00MB/s]\n",
      "197MB [01:03, 2.66MB/s]\n",
      "198MB [01:03, 3.47MB/s]\n",
      "199MB [01:04, 2.32MB/s]\n",
      "200MB [01:04, 3.06MB/s]\n",
      "201MB [01:04, 3.94MB/s]\n",
      "202MB [01:05, 2.46MB/s]\n",
      "203MB [01:05, 3.22MB/s]\n",
      "204MB [01:05, 4.12MB/s]\n",
      "205MB [01:06, 2.49MB/s]\n",
      "206MB [01:06, 3.26MB/s]\n",
      "207MB [01:06, 4.17MB/s]\n",
      "208MB [01:07, 2.04MB/s]\n",
      "209MB [01:07, 2.72MB/s]\n",
      "210MB [01:08, 2.19MB/s]\n",
      "212MB [01:08, 2.89MB/s]\n",
      "213MB [01:08, 3.74MB/s]\n",
      "214MB [01:09, 2.36MB/s]\n",
      "215MB [01:09, 3.10MB/s]\n",
      "216MB [01:09, 3.95MB/s]\n",
      "217MB [01:10, 2.41MB/s]\n",
      "218MB [01:10, 3.17MB/s]\n",
      "219MB [01:10, 4.05MB/s]\n",
      "220MB [01:11, 2.01MB/s]\n",
      "221MB [01:11, 2.68MB/s]\n",
      "222MB [01:11, 3.49MB/s]\n",
      "223MB [01:12, 2.32MB/s]\n",
      "224MB [01:12, 3.05MB/s]\n",
      "225MB [01:12, 3.93MB/s]\n",
      "226MB [01:13, 2.44MB/s]\n",
      "227MB [01:13, 3.21MB/s]\n",
      "229MB [01:13, 4.11MB/s]\n",
      "230MB [01:14, 2.53MB/s]\n",
      "231MB [01:14, 3.29MB/s]\n",
      "232MB [01:14, 4.23MB/s]\n",
      "233MB [01:15, 2.07MB/s]\n",
      "234MB [01:15, 2.76MB/s]\n",
      "235MB [01:16, 2.15MB/s]\n",
      "236MB [01:16, 2.85MB/s]\n",
      "237MB [01:16, 3.68MB/s]\n",
      "238MB [01:17, 2.33MB/s]\n",
      "239MB [01:17, 3.07MB/s]\n",
      "240MB [01:17, 3.95MB/s]\n",
      "241MB [01:18, 1.99MB/s]\n",
      "242MB [01:18, 3.23MB/s]\n",
      "\n",
      "0.00B [00:00, ?B/s]\n",
      "160kB [00:00, 2.60MB/s]\n"
     ]
    }
   ],
   "source": [
    "## Leave this cell untouched!\n",
    "\n",
    "!wget https://raw.githubusercontent.com/chentinghao/download_google_drive/master/download_gdrive.py\n",
    "!python download_gdrive.py 18ZNReFUt7RtxixQjDknN-A4qMiQQP04i ./pretrained_model.zip\n",
    "!python download_gdrive.py 1uIVoklkXeezsV35qXwZhA8eY0opSIgRA ./vocap.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iRSd1ojsjsr4"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-8661cddd69a4>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-8661cddd69a4>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    mkdir models\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Leave this cell untouched!\n",
    "%%bash\n",
    "mkdir models\n",
    "unzip -q pretrained_model.zip -d models\n",
    "unzip -q vocap.zip -d models\n",
    "wget https://farm9.staticflickr.com/8526/8697076784_afb6c9ef22_z.jpg -O image3.jpg\n",
    "wget https://farm5.staticflickr.com/4090/5200935390_bd4df7b05f_z.jpg -O image4.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgDWLc30n2FP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import argparse\n",
    "import pickle \n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms \n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "## Please DONOT remove these lines. \n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8a9V1nAob3d"
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE STARTS HERE ####\n",
    "# Check availability of GPU and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#### YOUR CODE ENDS HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qipnJ96UofrB"
   },
   "outputs": [],
   "source": [
    "# this is a simple wrapper for dealing with the vocabulary! \n",
    "# this'll be automatically used when you load the provided vocabulary file\n",
    "class Vocabulary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxhJn91XpCgz"
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # load the resnet152 network provided by torchvision\n",
    "        resnet = \n",
    "        # remove the last FC layer in the network\n",
    "        modules =\n",
    "        #define a sequential model with the modules obtained above\n",
    "        self.resnet =\n",
    "        # define a linear layer with out_features as embed_size\n",
    "        self.linear =\n",
    "        # define a 1d batch norm layer with embed_size as the number of features and a momentum of 0.01 \n",
    "        self.bn = \n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "    def forward(self, images):\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # calculate the forward pass in the order of base_model -> linear -> bn\n",
    "        # reshape features, if necessary\n",
    "        features = \n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        return features\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.max_seg_length = max_seq_length\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # define an embedding layer with vocab_size as the size of the dictionary\n",
    "        # and embed_size as the size of each embedding\n",
    "        self.embed =\n",
    "        # define a LSTM layer with embed_size, hidden_size, num_layers, set batch_first to True\n",
    "        self.lstm =\n",
    "        # define a linear layer with out features as the vocab size\n",
    "        self.linear =\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "    def forward(self, features, captions, lengths):\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # get embeddings for the captions\n",
    "        embeddings = \n",
    "        # concatenate the features with embeddings\n",
    "        embeddings = \n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features, states=None):\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(self.max_seg_length):\n",
    "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ph76BZtog56"
   },
   "outputs": [],
   "source": [
    "\n",
    "def main(img_path):\n",
    "    #### YOUR CODE STARTS HERE ####\n",
    "    # Image preprocessing\n",
    "    # define the transforms with normalization values: [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(), # convert to a tensor\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), # normalise the image\n",
    "                             (0.229, 0.224, 0.225))])\n",
    "    \n",
    "    with open('./models/vocab.pkl', 'rb') as f: # load the vocaulary wrapper file\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    # build encoder with a embed size of 256\n",
    "    encoder =\n",
    "    # build decoder with embed_size of 256, hidden_size of 512, vocab_size, 1 layer\n",
    "    decoder = \n",
    "\n",
    "    # move the encoder and decoder to device\n",
    "    encoder =\n",
    "    decoder =\n",
    "    \n",
    "    # load the pre-trained weights for encoder and decoder\n",
    "    encoder.\n",
    "    decoder.\n",
    "    #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "    # Prepare an image\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image = image.resize([224, 224], Image.LANCZOS)\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    image_tensor = image.to(device)\n",
    "    \n",
    "    # Generate an caption from the image\n",
    "    feature = encoder(image_tensor)\n",
    "    sampled_ids = decoder.sample(feature)\n",
    "    sampled_ids = sampled_ids[0].cpu().numpy()\n",
    "    \n",
    "    # Convert word_ids to words\n",
    "    sampled_caption = []\n",
    "    for word_id in sampled_ids:\n",
    "        word = vocab.idx2word[word_id]\n",
    "        sampled_caption.append(word)\n",
    "        if word == '<end>':\n",
    "            break\n",
    "    sentence = ' '.join(sampled_caption)\n",
    "    \n",
    "    # Print out the image and the generated caption\n",
    "    print (sentence)\n",
    "    image = Image.open(img_path)\n",
    "    plt.imshow(np.asarray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6g-PXhjqfDf"
   },
   "outputs": [],
   "source": [
    "main('image3.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSxLQza--HYv"
   },
   "source": [
    "#### Question-1\n",
    "\n",
    "Pick the caption displayed for image3.jpg\n",
    "1. a man looking at his phone\n",
    "2. a woman looking at his phone\n",
    "3. many people in front of a building \n",
    "4. a man and woman are standing in front of a building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpoAq05E8j26"
   },
   "outputs": [],
   "source": [
    "main('image4.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0sC2611-Xtx"
   },
   "source": [
    "#### Question-2\n",
    "\n",
    "Pick the caption displayed for image4.jpg\n",
    "1. a bunch of items that are on the floor\n",
    "2. an umbrella with a basket that are on the sidewalk \n",
    "3. a bunch of vegetables that are inside a shop\n",
    "4. a bunch of umbrellas that are sitting on a sidewalk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxd4_12r7uj8"
   },
   "source": [
    "### Part-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWojcPtKhXIG"
   },
   "source": [
    "This part is based on official PyTorch tutorial [Sequence-to-sequence modeling with nn.Transformer and TorchText](https://pytorch.org/tutorials/beginner/transformer_tutorial.html). Unlike the tutorial you'll have to code the encoding layer from scratch.\n",
    "\n",
    "**Note:**  We use a different set of hyper-parameters from the ones used in tutorial, ensure you only use the code in this notebook while reporting the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qYw8nrTrDoA"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdMzo-tMCPDa"
   },
   "outputs": [],
   "source": [
    "# You might need to restart your Jupyter runtime after running this cell\n",
    "\n",
    "!pip install torchtext==0.8.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7094dE1D2ZV"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)  \n",
    "\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRCD2izQd0VT"
   },
   "outputs": [],
   "source": [
    "class CustomEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=256, dropout=0.3, activation=\"relu\"):\n",
    "        super(CustomEncoderLayer, self).__init__()\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # define a multi head attention layer d_model, nhead and dropout\n",
    "        self.self_attn = \n",
    "        # define a linear layer with 512 as the number of output features\n",
    "        self.linear1 =\n",
    "        # define a dropout layer using the passed value\n",
    "        self.dropout =\n",
    "        # define a linear layer with dim_feedforward as the number of output features\n",
    "        self.linear2 = \n",
    "        # define a linear layer with d_model as the number of output features\n",
    "        self.linear3 =\n",
    "        # define a normalization layer (not batch and instance norm) with d_model as the normalized shape\n",
    "        self.norm1 =\n",
    "        # define a normalization layer (not batch and instance norm) with d_model as the normalized shape\n",
    "        self.norm2 =\n",
    "        # define a dropout layer using the passed value\n",
    "        self.dropout1 =\n",
    "        # define a dropout layer using the passed value\n",
    "        self.dropout2 =\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, src, src_mask, src_key_padding_mask):\n",
    "\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # use dropout1 of src2 and add to src\n",
    "        src = \n",
    "        # use the norm1 layer on src\n",
    "        src = \n",
    "        # use linear1 on src along with activation\n",
    "        src2 =\n",
    "        # use dropout on src2\n",
    "        src2 =\n",
    "        # use linear 2 on src2 along with activation\n",
    "        src2 =\n",
    "        # use linear3 on src2\n",
    "        src2 = \n",
    "        # use dropout2 on src2 and add it to src\n",
    "        src =\n",
    "        # use norm2 on src\n",
    "        src =\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXqkHLo8cQm3"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = CustomEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwGtR2tWcS0r"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.3, max_len=3000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOygFuYFcUe6"
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"spacy\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 64\n",
    "eval_batch_size = 32\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJSkWjeXcYHt"
   },
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfLntuy9cdey"
   },
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
    "emsize = 124 # embedding dimension\n",
    "nhid = 64 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 1 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 4 # the number of heads in the multiheadattention models\n",
    "dropout = 0.1 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCTAyGiFchUn"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZbeMt27cjcN"
   },
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 5 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFkiMXCquu4p"
   },
   "source": [
    "#### Question-3\n",
    "\n",
    "Report the final perplexity value (`ppl`) displayed (If you are not getting the exact number shown in options, please report the closest number).\n",
    "1. 151.45\n",
    "2. 174.21\n",
    "3. 148.91\n",
    "4. 123.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2lYxNv5EYGC"
   },
   "source": [
    "#Part-3\n",
    "# VAE\n",
    "Our VAE implementation will consist solely of fully connected layers. We'll take the 1 x 28 x 28 shape of our input and flatten the features to create an input dimension size of 784.\n",
    "\n",
    "In this section you'll define the Encoder and Decoder models, implement the reparametrization trick, forward pass, and loss function to train your first VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OSCGcPSEiow"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import numpy as np \n",
    "import random\n",
    "import argparse\n",
    "import pickle \n",
    "import os\n",
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.nn import init\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# for plotting\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxmelEsIcdqU"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)  \n",
    "\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ha8FNIPdUL7"
   },
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "#                                  BEGINNING OF YOUR CODE                                  #\n",
    "############################################################################################\n",
    "# TODO: Check availability of GPU and set the device accordingly                                 #\n",
    "device = None\n",
    "############################################################################################\n",
    "#                                  END OF YOUR CODE                                  #\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKl0wE6ZhyZU"
   },
   "outputs": [],
   "source": [
    "# Downloading the dataset\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "mnist_train = dset.MNIST('./MNIST_data', train=True, download=True,\n",
    "                           transform=T.ToTensor())\n",
    "loader_train = DataLoader(mnist_train, batch_size=batch_size,\n",
    "                          shuffle=True, drop_last=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oz2h0lcXy03J"
   },
   "outputs": [],
   "source": [
    "# Custom Function to show images\n",
    "\n",
    "def show_images(images):\n",
    "    images = torch.reshape(images, [images.shape[0], -1])  # images reshape to (batch_size, D)\n",
    "    sqrtn = int(math.ceil(math.sqrt(images.shape[0])))\n",
    "    sqrtimg = int(math.ceil(math.sqrt(images.shape[1])))\n",
    "\n",
    "    fig = plt.figure(figsize=(sqrtn, sqrtn))\n",
    "    gs = gridspec.GridSpec(sqrtn, sqrtn)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(img.reshape([sqrtimg,sqrtimg]))\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBQngPeQjLuM"
   },
   "source": [
    "## Define the model parameters\n",
    "\n",
    "1. Define the `encoder`, `decoder`, `mu_layer`, and `logvar_layer` in the initialization (`__init__` function) of the below class. Use nn.Sequential to define the encoder, and separate Linear layers for the mu and logvar layers. In all of these layers, H will be a hidden dimension you set and will be the same across all encoder and decoder layers. \n",
    "\n",
    "**Architecture for the encoder is described below:**\n",
    "\n",
    "\n",
    " * `Flatten` (Hint: nn.Flatten)\n",
    " * Fully connected layer with input size 784 (`input_size`) and output size H\n",
    " * `ReLU`\n",
    " * Fully connected layer with input_size H and output size H\n",
    " * `ReLU`\n",
    " * Fully connected layer with input_size H and output size H\n",
    " * `ReLU`\n",
    "\n",
    "\n",
    "We'll now define the decoder, which will take the latent space representation and generate a reconstructed image. The architecture is as follows: \n",
    "\n",
    "\n",
    " **Architecture for the decoder is described below:**\n",
    "\n",
    "\n",
    " * Fully connected layer with input size as the latent size (Z) and output size H\n",
    " * `ReLU`\n",
    " * Fully connected layer with input_size H and output size H\n",
    " * `ReLU`\n",
    " * Fully connected layer with input_size H and output size H\n",
    " * `ReLU`\n",
    " * Fully connected layer with input_size H and output size 784 (`input_size`)\n",
    " * `Sigmoid`\n",
    " * `Unflatten` (nn.Unflatten)\n",
    "\n",
    "\n",
    "Please do not touch the `forward` function for now. We will come back to it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqsz8iHCi03O"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, latent_size=15):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_size = input_size # H*W\n",
    "        self.latent_size = latent_size # Z\n",
    "        self.hidden_dim = 200\n",
    "\n",
    "        self.encoder = None\n",
    "        self.mu_layer = None\n",
    "        self.logvar_layer = None\n",
    "        self.decoder = None\n",
    "\n",
    "        ############################################################################################\n",
    "        #                                  BEGINNING OF YOUR CODE                                  #\n",
    "        ############################################################################################\n",
    "        ############################################################################################\n",
    "        # TODO: Implement the fully-connected encoder architecture described in the notebook.      #\n",
    "        # Specifically, self.encoder should be a network that inputs a batch of input images of    #\n",
    "        # shape (N, 1, H, W) into a batch of hidden features of shape (N, H_d). Set up             #\n",
    "        # self.mu_layer and self.logvar_layer to be a pair of linear layers that map the hidden    #\n",
    "        # features into estimates of the mean and log-variance of the posterior over the latent    #\n",
    "        # vectors; the mean and log-variance estimates will both be tensors of shape (N, Z).       #\n",
    "        ############################################################################################\n",
    "        # Replace \"pass\" statement with your code\n",
    "        pass\n",
    "        ############################################################################################\n",
    "        # TODO: Implement the fully-connected decoder architecture described in the notebook.      #\n",
    "        # Specifically, self.decoder should be a network that inputs a batch of latent vectors of  #\n",
    "        # shape (N, Z) and outputs a tensor of estimated images of shape (N, 1, H, W).             #\n",
    "        ############################################################################################\n",
    "        # Replace \"pass\" statement with your code\n",
    "        pass\n",
    "        ############################################################################################\n",
    "        #                                      END OF YOUR CODE                                    #\n",
    "        ############################################################################################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass through FC-VAE model by passing image through \n",
    "        encoder, reparametrize trick, and decoder models\n",
    "    \n",
    "        Inputs:\n",
    "        - x: Batch of input images of shape (N, 1, H, W)\n",
    "        \n",
    "        Returns:\n",
    "        - x_hat: Reconstruced input data of shape (N,1,H,W)\n",
    "        - mu: Matrix representing estimated posterior mu (N, Z), with Z latent space dimension\n",
    "        - logvar: Matrix representing estimataed variance in log-space (N, Z), with Z latent space dimension\n",
    "        \"\"\"\n",
    "        x_hat = None\n",
    "        mu = None\n",
    "        logvar = None\n",
    "\n",
    "        ############################################################################################\n",
    "        #                                  BEGINNING OF YOUR CODE                                  #\n",
    "        ############################################################################################\n",
    "        ############################################################################################\n",
    "        # TODO: Implement the forward pass by following these steps                                #\n",
    "        # (1) Pass the input batch through the encoder model, pass the output to mu_layer and      # \n",
    "        # logvar_layer to get the posterior mu and logvariance                                     #\n",
    "        # (2) Reparametrize to compute  the latent vector z                                        #\n",
    "        # (3) Pass z through the decoder to resconstruct x                                         #\n",
    "        ############################################################################################\n",
    "        # Replace \"pass\" statement with your code\n",
    "        pass\n",
    "        ############################################################################################\n",
    "        #                                      END OF YOUR CODE                                    #\n",
    "        ############################################################################################\n",
    "        return x_hat, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzNMeW1Ym2F2"
   },
   "source": [
    "## Reparameterization Trick\n",
    "\n",
    "Now we'll apply a reparametrization trick in order to estimate the posterior $z$ during our forward pass, given the $\\mu$ and $\\sigma^2$ estimated by the encoder. A simple way to do this could be to simply generate a normal distribution centered at our  $\\mu$ and having a std corresponding to our $\\sigma^2$. However, we would have to backpropogate through this random sampling that is not differentiable. Instead, we sample initial random data $\\epsilon$ from a fixed distrubtion, and compute $z$ as a function of ($\\epsilon$, $\\sigma^2$, $\\mu$). Specifically:\n",
    "\n",
    "$z = \\mu + \\sigma\\epsilon$\n",
    "\n",
    "We can easily find the partial derivatives w.r.t $\\mu$ and $\\sigma^2$ and backpropagate through $z$. If $\\epsilon = \\mathcal{N} (0,1)$, then its easy to verify that the result of our forward pass calculation will be a distribution centered at $\\mu$ with variance $\\sigma^2$.\n",
    "\n",
    "Implement the `reparametrization` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8C_bFtNejgbr"
   },
   "outputs": [],
   "source": [
    "def reparametrize(mu, logvar):\n",
    "    \"\"\"\n",
    "    Differentiably sample random Gaussian data with specified mean and variance using the\n",
    "    reparameterization trick.\n",
    "    Inputs:\n",
    "    - mu: Tensor of shape (N, Z) giving means\n",
    "    - logvar: Tensor of shape (N, Z) giving log-variances\n",
    "    Returns: \n",
    "    - z: Estimated latent vector of shape (N, Z), where z[i, j] is a random value sampled from a Gaussian with\n",
    "         mean mu[i, j] and log-variance logvar[i, j].\n",
    "    \"\"\"\n",
    "    z = None\n",
    "\n",
    "    ################################################################################################\n",
    "    #                                  BEGINNING OF YOUR CODE                                      #\n",
    "    ################################################################################################\n",
    "    ################################################################################################\n",
    "    # TODO: Reparametrize by initializing epsilon as a normal distribution and scaling by          #\n",
    "    # posterior mu and sigma to estimate z                                                         #\n",
    "    ################################################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    pass\n",
    "    ################################################################################################\n",
    "    #                              END OF YOUR CODE                                                #\n",
    "    ################################################################################################\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3RBCZL6nFWf"
   },
   "source": [
    "## Define the forward pass in the `VAE` class\n",
    "\n",
    "Go back to the cell above the Reparameterization trick header, which contains the definition of the `VAE` class.\n",
    "\n",
    "Implement the forward pass in the `forward` function, and run the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyMgNxsRnfH_"
   },
   "source": [
    "## Loss function\n",
    "\n",
    "The loss function for VAEs contains two terms: A reconstruction loss term (left) and KL divergence term (right). \n",
    "\n",
    "$-E_{Z~q_{\\phi}(z|x)}[log p_{\\theta}(x|z)] + D_{KL}(q_{\\phi}(z|x), p(z)))$\n",
    "\n",
    "Note that this is the negative of the variational lowerbound -this ensures that when we are minimizing this loss term, we're maximizing the variational lowerbound. The reconstruction loss term can be computed by simply using the binary cross entropy loss between the original input pixels and the output pixels of our decoder (Hint: `nn.functional.binary_cross_entropy`). The KL divergence term works to force the latent space distribution to be close to a prior distribution (we're using a standard normal gaussian as our prior).\n",
    "\n",
    "To help you out, we've derived an unvectorized form of the KL divergence term for you.\n",
    "Suppose that $q_\\phi(z|x)$ is a $Z$-dimensional diagonal Gaussian with mean $\\mu_{z|x}$ of shape $(Z,)$ and standard deviation $\\sigma_{z|x}$ of shape $(Z,)$, and that $p(z)$ is a $Z$-dimensional Gaussian with zero mean and unit variance. Then we can write the KL divergence term as:\n",
    "\n",
    "$D_{KL}(q_{\\phi}(z|x), p(z))) = -\\frac{1}{2} \\sum_{j=1}^{J} (1 + log(\\sigma_{z|x}^2)_{j} - (\\mu_{z|x})^2_{j} - (\\sigma_{z|x})^2_{j}$)\n",
    "\n",
    "Implement a vectorized version of this loss that operates on minibatches.\n",
    "You should average the loss across samples in the minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiWEliJll3KM"
   },
   "outputs": [],
   "source": [
    "def loss_function(x_hat, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    Computes the negative variational lower bound loss term of the VAE (refer to formulation in notebook).\n",
    "    Inputs:\n",
    "    - x_hat: Reconstruced input data of shape (N, 1, H, W)\n",
    "    - x: Input data for this timestep of shape (N, 1, H, W)\n",
    "    - mu: Matrix representing estimated posterior mu (N, Z), with Z latent space dimension\n",
    "    - logvar: Matrix representing estimated variance in log-space (N, Z), with Z latent space dimension\n",
    "    \n",
    "    Returns:\n",
    "    - loss: Tensor containing the scalar loss for the negative variational lowerbound\n",
    "    \"\"\"\n",
    "    loss = None\n",
    "    \n",
    "    ################################################################################################\n",
    "    #                                  BEGINNING OF YOUR CODE                                      #\n",
    "    ################################################################################################\n",
    "    ################################################################################################\n",
    "    # TODO: Compute negative variational lowerbound loss as described in the notebook.             #\n",
    "    # Note that the log variance is provided as the input to your function. Interpret the hints    #\n",
    "    # above accordingly.\n",
    "    ################################################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    pass    \n",
    "    ################################################################################################\n",
    "    #                            END OF YOUR CODE                                                  #\n",
    "    ################################################################################################\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2NahGjLwTWW"
   },
   "source": [
    "## Train the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8h0Qt5jVwr4L"
   },
   "outputs": [],
   "source": [
    "# Leave this cell untouched\n",
    "def train_vae(epoch, model, train_loader):\n",
    "    \"\"\"\n",
    "    Train the VAE!\n",
    "\n",
    "    Inputs:\n",
    "    - epoch: Current epoch number \n",
    "    - model: VAE model object\n",
    "    - train_loader: PyTorch Dataloader object that contains our training data\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    num_classes = 10\n",
    "    loss = None\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data\n",
    "        optimizer.step()\n",
    "    print('Train Epoch: {} \\tLoss: {:.6f}'.format(\n",
    "        epoch, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "slirT1GSwUXE"
   },
   "outputs": [],
   "source": [
    "# Leave this cell untouched\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "num_epochs = 10\n",
    "latent_size = 15\n",
    "input_size = 28*28\n",
    "\n",
    "\n",
    "vae_model = VAE(input_size, latent_size=latent_size)\n",
    "vae_model.to(device)\n",
    "for epoch in range(0, num_epochs):\n",
    "  train_vae(epoch, vae_model, loader_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhUx6MbPzXo-"
   },
   "source": [
    "## Question 4\n",
    "\n",
    "Report the final validation loss displayed after training the VAE, as displayed in the cell.\n",
    "\n",
    "1. 106.054\n",
    "2. 108.744\n",
    "3. 115.499\n",
    "4. 110.035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRU6mPsa3C05"
   },
   "source": [
    "# GANs\n",
    "\n",
    "We can think of GANs as a back and forth process of the generator ($G$) trying to fool the discriminator ($D$), and the discriminator trying to correctly classify real vs. fake as a minimax game:\n",
    "$$\\underset{G}{\\text{minimize}}\\; \\underset{D}{\\text{maximize}}\\; \\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] + \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
    "where $z \\sim p(z)$ are the random noise samples, $G(z)$ are the generated images using the neural network generator $G$, and $D$ is the output of the discriminator, specifying the probability of an input being real. \n",
    "<br><br>\n",
    "In [Goodfellow et al.](https://arxiv.org/abs/1406.2661), they analyze this minimax game and show how it relates to minimizing the Jensen-Shannon divergence between the training data distribution and the generated samples from $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BY0BKYT13y8-"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torchvision\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnMhc-Ab_syu"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)  \n",
    "\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjNw0exp4RAu"
   },
   "outputs": [],
   "source": [
    "def show_images(images):\n",
    "    images = torch.reshape(images, [images.shape[0], -1])  # images reshape to (batch_size, D)\n",
    "    sqrtn = int(math.ceil(math.sqrt(images.shape[0])))\n",
    "    sqrtimg = int(math.ceil(math.sqrt(images.shape[1])))\n",
    "\n",
    "    fig = plt.figure(figsize=(sqrtn, sqrtn))\n",
    "    gs = gridspec.GridSpec(sqrtn, sqrtn)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(img.reshape([sqrtimg,sqrtimg]))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKCzWX2u36_5"
   },
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "\n",
    "############################################################################################\n",
    "#                                  BEGINNING OF YOUR CODE                                  #\n",
    "############################################################################################\n",
    "# TODO: Check availability of GPU and set the device accordingly                                 #\n",
    "device = None\n",
    "############################################################################################\n",
    "#                                  END OF YOUR CODE                                  #\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIlAsdo74Dzm"
   },
   "outputs": [],
   "source": [
    "# let us load the input images from the dataset and visualize some images!\n",
    "\n",
    "batch_size = 128\n",
    "NOISE_DIM = 96\n",
    "\n",
    "print('download MNIST if not exist')\n",
    "\n",
    "mnist_train = dset.MNIST('./MNIST_data', train=True, download=True,\n",
    "                           transform=T.ToTensor())\n",
    "loader_train = DataLoader(mnist_train, batch_size=batch_size,\n",
    "                          shuffle=True, drop_last=True, num_workers=2)\n",
    "\n",
    "\n",
    "imgs = loader_train.__iter__().next()[0].view(batch_size, 784)\n",
    "show_images(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdjcIjLD4fhC"
   },
   "source": [
    "## Generating Random Noise\n",
    "\n",
    "The first step is to generate uniform noise from -1 to 1 with shape `[batch_size, noise_dim]`\n",
    "\n",
    "Hint: use `torch.rand`.\n",
    "\n",
    "Implement `sample_noise` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQLyyhiI4m0Q"
   },
   "outputs": [],
   "source": [
    "def sample_noise(batch_size, noise_dim, dtype=torch.float, device='cpu'):\n",
    "  \"\"\"\n",
    "  Generate a PyTorch Tensor of uniform random noise.\n",
    "\n",
    "  Input:\n",
    "  - batch_size: Integer giving the batch size of noise to generate.\n",
    "  - noise_dim: Integer giving the dimension of noise to generate.\n",
    "  \n",
    "  Output:\n",
    "  - A PyTorch Tensor of shape (batch_size, noise_dim) containing uniform\n",
    "    random noise in the range (-1, 1).\n",
    "  \"\"\"\n",
    "  noise = None\n",
    "  ##############################################################################\n",
    "  #                           BEGINNING OF YOUR CODE                           #\n",
    "  ##############################################################################\n",
    "  ##############################################################################\n",
    "  # TODO: Implement sample_noise.  \n",
    "  # The generated noise values (from uniform distribution) must be in\n",
    "  # the interval [-1,1].\n",
    "  # However, \"torch.rand\" generates random values in [0,1].\n",
    "  # For that, we must use a `transformation`, from [0,1] to [-1,1].\n",
    "  # -----\n",
    "  # The idea is given below (from: https://stackoverflow.com/a/44375813)\n",
    "  # If U is a random variable uniformly distributed on [0,1],\n",
    "  # then `(r1 - r2) * U + r2` is uniformly distributed on [r1,r2].\n",
    "  # -----\n",
    "  # In our case, `r1`=-1 and `r2`=1                                            \n",
    "  ##############################################################################\n",
    "  # Replace \"pass\" statement with your code\n",
    "  pass\n",
    "  ##############################################################################\n",
    "  #                              END OF YOUR CODE                              #\n",
    "  ##############################################################################\n",
    "\n",
    "  return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWvV_e6b5Aa_"
   },
   "source": [
    "## Generator and Discriminator\n",
    "\n",
    "**The definitions of these functions are provided to you.**\n",
    "\n",
    "They are explained below:\n",
    "\n",
    "### Discriminator\n",
    "\n",
    "The architecture is:\n",
    " * Fully connected layer with input size 784 and output size 256\n",
    " * LeakyReLU with alpha 0.01\n",
    " * Fully connected layer with input_size 256 and output size 256\n",
    " * LeakyReLU with alpha 0.01\n",
    " * Fully connected layer with input size 256 and output size 1\n",
    "  \n",
    "The output of the discriminator is of shape `[batch_size, 1]`, and contaisn real numbers corresponding to the scores that each of the `batch_size` inputs is a real image.\n",
    "\n",
    "### Generator\n",
    "\n",
    "The architecture is:\n",
    " * Fully connected layer from noise_dim to 1024\n",
    " * `ReLU`\n",
    " * Fully connected layer with size 1024 \n",
    " * `ReLU`\n",
    " * Fully connected layer with size 784\n",
    " * `TanH` (to clip the image to be in the range of [-1,1])\n",
    "\n",
    " This outputs a tensor of shape of `[batch_size, 784]` that is the image generated with the given sample noise to the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ifM_OGKb5Mts"
   },
   "outputs": [],
   "source": [
    "NOISE_DIM = 96 # default noise dimension\n",
    "\n",
    "def discriminator():\n",
    "  \"\"\"\n",
    "  Build and return a PyTorch nn.Sequential model implementing the architecture in the notebook.\n",
    "  \"\"\"\n",
    "\n",
    "  model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(784, 256),  # 1st Fully-Connected layer.\n",
    "    nn.LeakyReLU(0.01),\n",
    "    nn.Linear(256, 256),  # 2nd Fully-Connected layer.\n",
    "    nn.LeakyReLU(0.01),\n",
    "    nn.Linear(256, 1)     # 3rd Fully-Connected layer.\n",
    "  )\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def generator(noise_dim=NOISE_DIM):\n",
    "  \"\"\"\n",
    "  Build and return a PyTorch nn.Sequential model implementing the architecture in the notebook.\n",
    "  \"\"\"\n",
    "  model = nn.Sequential(\n",
    "    nn.Linear(noise_dim, 1024),  # 1st Fully-Connected layer.\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 1024),       # 2nd Fully-Connected layer.\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 784),        # 3rd Fully-Connected layer.\n",
    "    nn.Tanh()\n",
    "  )\n",
    "\n",
    "  return model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkYTW6UF5oKQ"
   },
   "source": [
    "## Loss functions\n",
    "\n",
    "This is the most important part. Please read the instructions below carefully.\n",
    "\n",
    "Compute the generator and discriminator loss. The generator loss is:\n",
    "$$\\ell_G  =  -\\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right]$$\n",
    "and the discriminator loss is:\n",
    "$$ \\ell_D = -\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] - \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
    "We will be *minimizing* these losses.\n",
    "\n",
    "For the purpose of these equations, we assume that the output from the discriminator is a real number in the range $0 < D(x) < 1$ which results from squashing the raw score from the discriminator through a sigmoid function. However for a cleaner and more numerically stable implementation, we have not included the sigmoid in the discriminator architecture above -- instead we will implement the sigmoid as part of the loss function.\n",
    "\n",
    "**HINTS**: You can use the function [`torch.nn.functional.binary_cross_entropy_with_logits`](https://pytorch.org/docs/stable/nn.functional.html#binary-cross-entropy-with-logits) to compute these losses in a numerically stable manner.\n",
    "\n",
    "Given a score $s\\in\\mathbb{R}$ and a label $y\\in\\{0, 1\\}$, the binary cross entropy loss (with logits) is defined as:\n",
    "\n",
    "$$ bce(s, y) = -y * \\log(\\sigma(s)) - (1 - y) * \\log(1 - \\sigma(s)) $$\n",
    "\n",
    "where $\\sigma(s)=1/(1+\\exp(-s))$ is the sigmoid function.\n",
    "\n",
    "A naive implementation of this formula can be numerically unstable, so you should prefer to use the built-in PyTorch implementation.\n",
    "\n",
    "You will also need to compute labels corresponding to real or fake and use the logit arguments to determine their size. Make sure you cast these labels to the correct data type using the global `dtype` variable, for example:\n",
    "\n",
    "`true_labels = torch.ones(size, device=device)`\n",
    "\n",
    "Instead of computing the expectation of $\\log D(G(z))$, $\\log D(x)$ and $\\log \\left(1-D(G(z))\\right)$, we will be averaging over elements of the minibatch, so make sure to combine the loss by averaging instead of summing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_Jjl_Me5pnf"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(logits_real, logits_fake):\n",
    "  \"\"\"\n",
    "  Computes the discriminator loss described above.\n",
    "  \n",
    "  Inputs:\n",
    "  - logits_real: PyTorch Tensor of shape (N,) giving scores for the real data.\n",
    "  - logits_fake: PyTorch Tensor of shape (N,) giving scores for the fake data.\n",
    "  \n",
    "  Returns:\n",
    "  - loss: PyTorch Tensor containing (scalar) the loss for the discriminator.\n",
    "  \"\"\"\n",
    "  loss = None\n",
    "  ##############################################################################\n",
    "  #                           BEGINNING OF YOUR CODE                           #\n",
    "  ##############################################################################\n",
    "  ##############################################################################\n",
    "  # TODO: Implement discriminator_loss.                                        #\n",
    "  ##############################################################################\n",
    "\n",
    "  # For the discriminator (D), the true target (y = 1) corresponds to \"real\" images.\n",
    "  # Thus, for the scores of real images, the target is always 1 (a vector).\n",
    "  real_labels = \n",
    "  # Compute the BCE for the scores of the real images.\n",
    "  # Note that the BCE itself uses the Expectation formula (in addition, an average is\n",
    "  # taken throughout the losses, not a sum [as requested in this assignment]).\n",
    "  real_loss = \n",
    "\n",
    "  # For D, the false target (y = 0) corresponds to \"fake\" images.\n",
    "  # Thus, for the scores of fake images, the target is always 0 (a vector).\n",
    "  fake_labels = \n",
    "  # As for the real scores, compute the BCE loss for the fake images.\n",
    "  fake_loss = \n",
    "\n",
    "  # Sum \"real\" and \"fake\" losses.\n",
    "  # That is, BCE has already taken into account the \"negated equation\" form,\n",
    "  # the \"log\" (in the Expectation) and the \"mean\" (instead on the \"sum\").\n",
    "  loss = \n",
    "\n",
    "  ##############################################################################\n",
    "  #                              END OF YOUR CODE                              #\n",
    "  ##############################################################################\n",
    "  return loss\n",
    "\n",
    "def generator_loss(logits_fake):\n",
    "  \"\"\"\n",
    "  Computes the generator loss described above.\n",
    "\n",
    "  Inputs:\n",
    "  - logits_fake: PyTorch Tensor of shape (N,) giving scores for the fake data.\n",
    "  \n",
    "  Returns:\n",
    "  - loss: PyTorch Tensor containing the (scalar) loss for the generator.\n",
    "  \"\"\"\n",
    "  loss = None\n",
    "  ##############################################################################\n",
    "  #                           BEGINNING OF YOUR CODE                           #\n",
    "  ##############################################################################\n",
    "  ##############################################################################\n",
    "  # TODO: Implement generator_loss.                                            #\n",
    "  ##############################################################################\n",
    "\n",
    "  # For the generator (G), the true target (y = 1) corresponds to \"fake\" images.\n",
    "  # Thus, for the scores of fake images, the target is always 1 (a vector).\n",
    "  fake_labels =\n",
    "  # Compute the BCE for the scores of the fake images.\n",
    "  fake_loss =\n",
    "\n",
    "  # The generator loss is \"fake_loss\".\n",
    "  # That is, BCE has already taken into account the \"negated equation\" form,\n",
    "  # the \"log\" (in the Expectation) and the \"mean\" (insetead on the \"sum\").\n",
    "  loss =\n",
    "\n",
    "  ##############################################################################\n",
    "  #                              END OF YOUR CODE                              #\n",
    "  ##############################################################################\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3zkv8Mp69Kx"
   },
   "source": [
    "## Training\n",
    "\n",
    "We provide you the main training loop... you won't need to change this function, but we encourage you to read through and understand it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OW51x6_Q7Ddq"
   },
   "outputs": [],
   "source": [
    "def run_a_gan(D, G, D_solver, G_solver, discriminator_loss, generator_loss, save_filename, show_every=250, \n",
    "              batch_size=128, noise_size=96, num_epochs=10):\n",
    "  \"\"\"\n",
    "  Train a GAN!\n",
    "  \n",
    "  Inputs:\n",
    "  - D, G: PyTorch models for the discriminator and generator\n",
    "  - D_solver, G_solver: torch.optim Optimizers to use for training the\n",
    "    discriminator and generator.\n",
    "  - discriminator_loss, generator_loss: Functions to use for computing the generator and\n",
    "    discriminator loss, respectively.\n",
    "  - show_every: Show samples after every show_every iterations.\n",
    "  - batch_size: Batch size to use for training.\n",
    "  - noise_size: Dimension of the noise to use as input to the generator.\n",
    "  - num_epochs: Number of epochs over the training dataset to use for training.\n",
    "  \"\"\"\n",
    "  iter_count = 0\n",
    "  for epoch in range(num_epochs):\n",
    "    for x, _ in loader_train:\n",
    "      if len(x) != batch_size:\n",
    "        continue\n",
    "      D_solver.zero_grad()\n",
    "      real_data = x.view(-1, 784).to(device)\n",
    "      logits_real = D(2* (real_data - 0.5))\n",
    "\n",
    "      g_fake_seed = sample_noise(batch_size, noise_size, dtype=real_data.dtype, device=real_data.device)\n",
    "      fake_images = G(g_fake_seed).detach()\n",
    "      logits_fake = D(fake_images)\n",
    "\n",
    "      d_total_error = discriminator_loss(logits_real, logits_fake)\n",
    "      d_total_error.backward()        \n",
    "      D_solver.step()\n",
    "\n",
    "      G_solver.zero_grad()\n",
    "      g_fake_seed = sample_noise(batch_size, noise_size, dtype=real_data.dtype, device=real_data.device)\n",
    "      fake_images = G(g_fake_seed)\n",
    "\n",
    "      gen_logits_fake = D(fake_images)\n",
    "      g_error = generator_loss(gen_logits_fake)\n",
    "      g_error.backward()\n",
    "      G_solver.step()\n",
    "\n",
    "      if (iter_count % show_every == 0):\n",
    "        print('Iter: {}, D: {:.4}, G:{:.4}'.format(iter_count,d_total_error.item(),g_error.item()))\n",
    "        imgs_numpy = fake_images.data.cpu()#.numpy()\n",
    "        show_images(imgs_numpy[0:16])\n",
    "        plt.show()\n",
    "        print()\n",
    "      iter_count += 1\n",
    "    if epoch == num_epochs - 1:\n",
    "      show_images(imgs_numpy[0:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo1ajReC7NA-"
   },
   "source": [
    "Now run the cell below to train your first GAN! Your last epoch results will be stored in `fc_gan_results.jpg` for visualization.\n",
    "\n",
    "In the iterations in the low 100s you should see black backgrounds, fuzzy shapes as you approach iteration 1000, and decent shapes, about half of which will be sharp and clearly recognizable as we pass 3000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHZM8Ofc7DlF"
   },
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "\n",
    "# Make the discriminator\n",
    "D = discriminator().to(device)\n",
    "\n",
    "# Make the generator\n",
    "G = generator().to(device)\n",
    "\n",
    "# Use the function you wrote earlier to get optimizers for the Discriminator and the Generator\n",
    "D_solver = optim.Adam(D.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "G_solver = optim.Adam(G.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "# Run it!\n",
    "run_a_gan(D, G, D_solver, G_solver, discriminator_loss, generator_loss, 'fc_gan_results.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1fKq9oU1ZKy"
   },
   "source": [
    "## Question 5\n",
    "\n",
    "What is the discriminator error after 4500 iterations displayed in the cell? (choose the closest answer)\n",
    "\n",
    "1. 1.303\n",
    "2. 1.450\n",
    "3. 0.352\n",
    "4. 0.298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FPSx91R7sKW"
   },
   "outputs": [],
   "source": [
    "set_seed(4)\n",
    "\n",
    "g_fake_seed = sample_noise(64, 96, dtype=dtype, device=device)\n",
    "fake_images = G(g_fake_seed)\n",
    "\n",
    "gen_logits_fake = D(fake_images)\n",
    "g_error = generator_loss(gen_logits_fake)\n",
    "\n",
    "print(g_error.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O72WWjjQ7gMP"
   },
   "source": [
    "## Question 6\n",
    "\n",
    "What is the generator error displayed in the cell? (choose the closest answer)\n",
    "\n",
    "1. 0.7431\n",
    "2. 0.8152\n",
    "3. 0.7815\n",
    "4. 0.7611\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McdGRA8A1BGn"
   },
   "source": [
    "# Normalizing Flows [Optional]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9mA1O4b8yxr"
   },
   "source": [
    "\n",
    "<script type=\"text/javascript\" async\n",
    "src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
    "</script>\n",
    "\n",
    "<script type=\"text/x-mathjax-config\">\n",
    "    MathJax.Hub.Config({\n",
    "    tex2jax: {\n",
    "    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
    "    processEscapes: true\n",
    "    }\n",
    "    });\n",
    "</script>\n",
    "\n",
    "<p><em>Normalizing flows</em> provide a mechanism to transform simple distributions into more complex ones without sacrificing the computational conveniences that make the former appealing and practical.</p>\n",
    "<p>The idea was introduced in its popular form to the machine learning community in <a href=\"https://arxiv.org/abs/1505.05770\">Rezende et al.</a></p>\n",
    "\n",
    "\n",
    "\n",
    "<h2 id=\"normalizing-flows\">Overview</h2>\n",
    "<p>The basic idea is to pass a sample from a simple base distribution through a series of transformations or <em>flows</em>. The transformations $f$ must be smooth, differentiable ($f$ and $f^-1$) and invertible. Functions of this type are known as diffeomorphisms. The transformed sample can now be thought of as a sample from a more complex distribution. The invertibility condition gives a closed form description of the relationship between the density of the initial distribution and the final.</p>\n",
    "<p>Consider an invertible smooth mapping $$f: \\mathcal{R}^{d} \\to \\mathcal{R}^d$$ with an inverse $f^{-1} = g$.</p>\n",
    "<p>Given a random variable $\\mathbb{z}$ with distribution $q(\\mathbb{z})$, $\\mathbb{z}^{'} = f(\\mathbb{z})$ has the distribution:</p>\n",
    "<p>$$q(\\mathbb{z}') = q(\\mathbb{z}) |det \\frac{\\partial f^{-1}}{\\partial \\mathbb{z}^{'}} | = q(\\mathbb{z})| det \\frac{\\partial f}{\\partial \\mathbb{z}} |^{-1}$$</p>\n",
    "<p>Additionally if we use $k$ flows and transform our initial sample $\\mathbb{z}_0$: $\\mathbb{z}_k= f_K \\circ … \\circ f_2 \\circ f_1(\\mathbb{z}_0)$</p>\n",
    "<p>We can get the final log-density $\\ln(q_k(\\mathbb{z}))$ as follows:</p>\n",
    "<p>$\\ln q_K(\\mathbb{z}_K)= \\ln_{q_0}(\\mathbb{z}_0)-\\sum_{k=1}^K \\ln \\lvert det \\frac{\\partial f_k}{\\partial \\mathbb{z}_{{k-1}}}\\rvert$</p>\n",
    "<p>Nice!</p>\n",
    "<p>Now all we need is an invertible transformation to use. The authors propose a family of transformations called <em>planar flows</em> which take the form: $$f(z)=z+uh(w^T z + b)$$ Where:</p>\n",
    "<ul>\n",
    "<li>$\\mathbb{w} \\in \\mathcal{R}^D$</li>\n",
    "<li>$\\mathbb{u} \\in \\mathcal{R}^D$</li>\n",
    "<li>$b \\in \\mathcal{R}$</li>\n",
    "<li>$h$ is an elementwise nonlinearity (like <em>tanh</em>).</li>\n",
    "</ul>\n",
    "<p>The Determinant-Jacobian can be computed efficiently:</p>\n",
    "<p>$$\\psi(\\mathbb{z}) = h’(\\mathbb{w}^Tz + b)\\mathbb{w}$$ $$| det \\frac{\\partial f}{\\partial \\mathbb{z}}| = \\lvert 1+ \\mathbb{u}^T \\psi(\\mathbb{z}) \\rvert $$</p>\n",
    "<p>Our formula for the final log density is:</p>\n",
    "<p>$$\\ln q_K(\\mathbb{z}_K) = \\ln_{q_0}(\\mathbb{z}) - \\sum_{k=1}^K \\ln \\lvert 1 + \\mathbb{u}_k^T \\psi_k(\\mathbb{z}_{k-1})\\rvert$$</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GnJOYiu-eP0"
   },
   "outputs": [],
   "source": [
    "# Do not modify this code\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)  \n",
    "\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hL9vWBqw1EhV"
   },
   "outputs": [],
   "source": [
    "class PlanarFlow(nn.Module):\n",
    "    \"\"\"\n",
    "    A single planar flow, computes T(x) and log(det(jac_T)))\n",
    "    \"\"\"\n",
    "    def __init__(self, D):\n",
    "        super(PlanarFlow, self).__init__()\n",
    "        self.u = nn.Parameter(torch.Tensor(1, D), requires_grad=True)\n",
    "        self.w = nn.Parameter(torch.Tensor(1, D), requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.Tensor(1), requires_grad=True)\n",
    "        self.h = torch.tanh\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        self.w.data.uniform_(-0.01, 0.01)\n",
    "        self.b.data.uniform_(-0.01, 0.01)\n",
    "        self.u.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "    def forward(self, z):\n",
    "\n",
    "        f_output = None\n",
    "\n",
    "        ################################################################################################\n",
    "        #                                  BEGINNING OF YOUR CODE                                      #  \n",
    "        ################################################################################################\n",
    "        ################################################################################################\n",
    "        # TODO: Implement the forward pass denoted by f(z). Store the result in f_output.              #\n",
    "        ################################################################################################\n",
    "        # Replace \"pass\" statement with your code\n",
    "        pass\n",
    "\n",
    "        # matrix-multiply z and w, and add b\n",
    "        linear_term = \n",
    "        # add z to u * h(linear_term)\n",
    "        f_output = \n",
    "\n",
    "        ################################################################################################\n",
    "        #                                     END OF YOUR CODE                                         #\n",
    "        ################################################################################################\n",
    "        return f_output\n",
    "\n",
    "    def h_prime(self, x):\n",
    "        \"\"\"\n",
    "        Derivative of tanh\n",
    "        \"\"\"\n",
    "        return (1 - self.h(x) ** 2)\n",
    "\n",
    "    def psi(self, z):\n",
    "\n",
    "        psi_output = None\n",
    "\n",
    "        ################################################################################################\n",
    "        #                                  BEGINNING OF YOUR CODE                                      #  \n",
    "        ################################################################################################\n",
    "        ################################################################################################\n",
    "        # TODO: Implement the function psi(z). Store the result in psi_output.                         #\n",
    "        ################################################################################################\n",
    "        # Replace \"pass\" statement with your code\n",
    "        pass\n",
    "\n",
    "        # matrix-multiply z and w, add b\n",
    "        inner = \n",
    "\n",
    "        # multiply w to h_prime(inner)\n",
    "        psi_output = \n",
    "\n",
    "        ################################################################################################\n",
    "        #                                     END OF YOUR CODE                                         #\n",
    "        ################################################################################################\n",
    "        return psi_output\n",
    "        \n",
    "\n",
    "    def log_det(self, z):\n",
    "\n",
    "        log_det_output = None\n",
    "\n",
    "        ################################################################################################\n",
    "        #                                  BEGINNING OF YOUR CODE                                      #  \n",
    "        ################################################################################################\n",
    "        ################################################################################################\n",
    "        # TODO: Implement the logarithm of the determinant Jacobian.                                   #\n",
    "        # Store the result in log_det_output.                                                          #\n",
    "        # Note: Take the absolute value of the determinant, before applying logarithm                  #\n",
    "        ################################################################################################\n",
    "        # Replace \"pass\" statement with your code\n",
    "        pass\n",
    "\n",
    "        # calculate inner as 1 + matrix-multiply(psi(z), u)\n",
    "        inner = \n",
    "\n",
    "        # calculate log_det_output as log(|inner|)\n",
    "        log_det_output = \n",
    "        ################################################################################################\n",
    "        #                                     END OF YOUR CODE                                         #\n",
    "        ################################################################################################\n",
    "\n",
    "        return log_det_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLkXMg-k-Pek"
   },
   "outputs": [],
   "source": [
    "class NormalizingFlow(nn.Module):\n",
    "    \"\"\"\n",
    "    A normalizing flow composed of a sequence of planar flows.\n",
    "    \"\"\"\n",
    "    def __init__(self, D, n_flows=2):\n",
    "        super(NormalizingFlow, self).__init__()\n",
    "        self.flows = nn.ModuleList(\n",
    "            [PlanarFlow(D) for _ in range(n_flows)])\n",
    "\n",
    "    def sample(self, base_samples):\n",
    "        \"\"\"\n",
    "        Transform samples from a simple base distribution\n",
    "        by passing them through a sequence of Planar flows.\n",
    "        \"\"\"\n",
    "        samples = base_samples\n",
    "        for flow in self.flows:\n",
    "            samples = flow(samples)\n",
    "        return samples\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes and returns the sum of log_det_jacobians\n",
    "        and the transformed samples T(x).\n",
    "        \"\"\"\n",
    "        sum_log_det = 0\n",
    "        transformed_sample = x\n",
    "\n",
    "        for i in range(len(self.flows)):\n",
    "            log_det_i = (self.flows[i].log_det(transformed_sample))\n",
    "            sum_log_det += log_det_i\n",
    "            transformed_sample = self.flows[i](transformed_sample)\n",
    "\n",
    "        return transformed_sample, sum_log_det\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNvVyLe3_E2r"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Potential functions U(x) from Rezende et al. 2015\n",
    "p(z) is then proportional to exp(-U(x)).\n",
    "Since we log this value later in the optimized bound,\n",
    "no need to actually exp().\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def w_1(z):\n",
    "    return torch.sin((2 * math.pi * z[:, 0]) / 4)\n",
    "\n",
    "\n",
    "def w_2(z):\n",
    "    return 3 * torch.exp(-.5 * ((z[:, 0] - 1) / .6) ** 2)\n",
    "\n",
    "\n",
    "def sigma(x):\n",
    "    return 1 / (1 + torch.exp(- x))\n",
    "\n",
    "\n",
    "def w_3(z):\n",
    "    return 3 * sigma((z[:, 0] - 1) / .3)\n",
    "\n",
    "\n",
    "def pot_1(z):\n",
    "    z_1, z_2 = z[:, 0], z[:, 1]\n",
    "    norm = torch.sqrt(z_1 ** 2 + z_2 ** 2)\n",
    "    outer_term_1 = .5 * ((norm - 2) / .4) ** 2\n",
    "    inner_term_1 = torch.exp((-.5 * ((z_1 - 2) / .6) ** 2))\n",
    "    inner_term_2 = torch.exp((-.5 * ((z_1 + 2) / .6) ** 2))\n",
    "    outer_term_2 = torch.log(inner_term_1 + inner_term_2 + 1e-7)\n",
    "    u = outer_term_1 - outer_term_2\n",
    "    return - u\n",
    "\n",
    "\n",
    "def pot_2(z):\n",
    "    u = .5 * ((z[:, 1] - w_1(z)) / .4) ** 2\n",
    "    return - u\n",
    "\n",
    "\n",
    "def pot_3(z):\n",
    "    term_1 = torch.exp(-.5 * (\n",
    "        (z[:, 1] - w_1(z)) / .35) ** 2)\n",
    "    term_2 = torch.exp(-.5 * (\n",
    "        (z[:, 1] - w_1(z) + w_2(z)) / .35) ** 2)\n",
    "    u = - torch.log(term_1 + term_2 + 1e-7)\n",
    "    return - u\n",
    "\n",
    "\n",
    "def pot_4(z):\n",
    "    term_1 = torch.exp(-.5 * ((z[:, 1] - w_1(z)) / .4) ** 2)\n",
    "    term_2 = torch.exp(-.5 * ((z[:, 1] - w_1(z) + w_3(z)) / .35) ** 2)\n",
    "    u = - torch.log(term_1 + term_2)\n",
    "    return - u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1RViF9S-RSR"
   },
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "\n",
    "def random_normal_samples(n, dim=2):\n",
    "    return torch.zeros(n, dim).normal_(mean=0, std=1)\n",
    "\n",
    "# we are using 16 planar flows\n",
    "model = NormalizingFlow(2, 16)\n",
    "\n",
    "# target density as pot1\n",
    "target_density = pot_1\n",
    "\n",
    "# RMSprop is what they used in renzende et al\n",
    "opt = torch.optim.RMSprop(\n",
    "    params=model.parameters(),\n",
    "    lr=1e-4,\n",
    "    momentum=0.9\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', patience=1000)\n",
    "losses = []\n",
    "\n",
    "for iter_ in range(10000):\n",
    "    if iter_ % 100 == 0:\n",
    "        print(\"Iteration {}\".format(iter_), end=\": \")\n",
    "\n",
    "    samples = Variable(random_normal_samples(100))\n",
    "\n",
    "    z_k, sum_log_det = model(samples)\n",
    "    log_p_x = target_density(z_k)\n",
    "\n",
    "    # Reverse KL since we can evaluate target density but can't sample\n",
    "    loss = (- sum_log_det - (log_p_x)).mean()\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    scheduler.step(loss)\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if iter_ % 100 == 0:\n",
    "        print(\"Loss {}\".format(loss.item())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRKUX077F2Uf"
   },
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "\n",
    "samples = Variable(random_normal_samples(100))\n",
    "\n",
    "z_k, sum_log_det = model(samples)\n",
    "log_p_x = target_density(z_k)\n",
    "\n",
    "print(sum_log_det.mean().item())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL4V-Assignment-5-Week-9&10-Questions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
